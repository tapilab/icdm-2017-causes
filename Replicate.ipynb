{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Entities' cause commitment in public messaging\n",
    "This notebook replicates the analysis in the paper: Zhao Wang, Jennifer Cutler, Aron Culotta. [\"Are Words Commensurate with Actions? Quantifying Commitment to A Cause from Online Public Messaging\"](www.google.com) IEEE International Conference on Data Mining Workshops (ACUMEN: Data science for human performance in social networks), 2017.<br><br>\n",
    "The goal of this analysis is to identify potential \"inauthentic\" entities by comparing how do entities show commitment toward a cause in their tweets and in their actions.<br><br>\n",
    "This notebook has been tested in **Python 3.4.3** only.<br><br>\n",
    "The code below is exactly what was used to produce the tables and figures in the final version of the paper. See requirements.txt for the versions of external libraries used.<br><br>\n",
    "**Data**<br>\n",
    "Please see the paper for the details of data collection. This notebook assumes access to the pre-collected data, which is available here:\n",
    "[link to dropbox]<br>\n",
    "Please contact Zhao (zwang185@hawk.iit.edu) for access.<br>\n",
    "This is about [1GB]. Once you download this data, place it in a folder called data, in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 7 sections of implementation and analysis:<br>\n",
    "**Section 1: select cause-relevant tweets as training data<br>**\n",
    "> 1.1 Get entities' information<br>\n",
    "> 1.2 Read tweets for entities<br>\n",
    "> 1.3 Score and sort tweets by cause-relevance<br>\n",
    "> 1.4 Select and label each entity's top-n relevant tweets as training data<br>\n",
    "\n",
    "**Section 2: feature engineering**\n",
    "- 2.1 Linguistic features<br>\n",
    ">2.1.1 Sentiment polarity<br>\n",
    ">2.1.2 Pronouns <br>\n",
    ">2.1.3 Cause keywords and Context of cause keywords <br>\n",
    ">2.1.4 Social interactions <br>\n",
    ">2.1.5 Part-Of-Speach tag <br>\n",
    "- 2.2 Word embedding features<br>\n",
    ">2.2.1 Tweet vector and tweet cause relevance score<br>\n",
    ">2.2.2 Top-n(n=3,5) words, top-n words' vector and top-n words' cause relevance scores<br>\n",
    ">2.2.3 Cause keywords(sim>=0.30), number of cause keywords, cause keywords' vector, cause words' relevance scores<br>\n",
    ">2.2.4 Context words (window = 1), context vector, context words' contribution scores<br>\n",
    "\n",
    "**Section 3: train and evaluate support classifier with manually labeled tweets**\n",
    ">3.1 Evaluating linguistic features, word embedding features and combination of various features<br>\n",
    ">3.2 Evaluate different classifiers<br>\n",
    ">3.3 Analyze terms that have high coefficients<br>\n",
    "\n",
    "**Section 4: train and evaluate commitment classifier with manually labeled tweets**\n",
    ">4.1 Evaluating linguistic features, word embedding features and combination of various features<br>\n",
    ">4.2 Evaluating different classifiers<br>\n",
    ">4.3 Analyze terms that have high coefficients<br>\n",
    "\n",
    "**Secrtion 5: apply pre-trained classifiers to predict for unseen tweets**\n",
    ">5.1 Apply support classifier to classify all brands' tweets into support and non-support classes<br>\n",
    ">5.2 Apply commitment classifier to classify all brands' support tweets into high- and low- commitment classes<br>\n",
    "\n",
    "**Section 6: aggregate each entity's cause-commitment tweets and compare with action score to find inauthentic entities**\n",
    ">6.1 Apply different aggregation methods to select entities that have high word-ratings<br>\n",
    ">6.2 Sort high word-rating entities by their action-rating and select top-n(high word-rating but low action-rating) as inauthentic entities\n",
    "\n",
    "**7.Fit linear regression model to analyze how does entities' word commitment level relate with action-ratings**<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Implementation\n",
    "There are 3 datasets with entity&cause pairs: brands with health cause, brands with eco cause, member of congress (moc for short) with eco cause. The code will implement 7 sections for each dataset separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import Cause\n",
    "import nltk, logging, re, operator\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BRAND_PATH = '/data/2/lip_service/recollect2/'\n",
    "ECO_SCORE = '/data/2/lip_service/recollect2/brands.csv'\n",
    "HEALTH_SCORE = '/data/2/zwang/2017_S/Tweet_Health/goodguide_health.csv'\n",
    "CONGRESS_PATH = '/data/2/zwang/congress/congress_pruned.json'\n",
    "W2V_PATH = '/data/2/zwang/brand_score/GoogleNews/GoogleNews-vectors-negative300.bin'\n",
    "eco_term_path = '/data/2/lip_service/eco_terms.txt'\n",
    "brand_eco_path = '/data/2/zwang/2017_S/Tweet_Eco/brand_eco_labeled_tweets.csv'\n",
    "brand_health_path = '/data/2/zwang/2017_S/Tweet_Health/brand_health_labeled_tweets.csv'\n",
    "moc_eco_path = '/data/2/zwang/2017_S/Congress_Eco/congress_eco_labeled_tweets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eco_cause = \"environment\"\n",
    "eco_keywords =[\"environment\",\"ecosystem\",\"biodiversity\",\"habitats\",\"climate\",\"ecology\",\"plantlife\",\"pollution\",\"rainforests\"]\n",
    "\n",
    "health_cause = \"health\"\n",
    "health_keywords =[\"healthy\",\"nutritious\",\"lowfat\",\"wholesome\",\"organic\",\"natural\",\"vegan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eco_terms = Cause.read_eco_terms(eco_term_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mycv=KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-21 10:51:43,847 INFO loading projection weights from /data/2/zwang/brand_score/GoogleNews/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading GoogleNews word2vec model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-09-21 10:52:46,552 INFO loaded (3000000, 300) matrix from /data/2/zwang/brand_score/GoogleNews/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is: 3000000\n"
     ]
    }
   ],
   "source": [
    "GN_model = Cause.load_GoogleNews_w2v(W2V_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Dataset 1: brands with health cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 1: select cause-relevant tweets as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 169 brands with health score\n"
     ]
    }
   ],
   "source": [
    "healthbrand_score_dict, healthbrand_sector = Cause.get_brand_info(HEALTH_SCORE,\"screen_name\",\"health score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "healthbrand_nameid = Cause.get_healthbrand_nameid(ECO_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector distribution of health brands:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'Food': 106, 'Personal Care': 63})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sector distribution of health brands:\")\n",
    "health_sector = Counter()\n",
    "health_sector.update(healthbrand_sector.values())\n",
    "health_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 500000 lines\n",
      "read 1000000 lines\n",
      "read 1500000 lines\n",
      "read 2000000 lines\n",
      "read 2500000 lines\n",
      "Collected 429009 tweets for 142 health brands in total.\n"
     ]
    }
   ],
   "source": [
    "healthbrand_tweets_dict = Cause.read_brand_tweets(BRAND_PATH+\"tweets.pruned.json.gz\", list(healthbrand_score_dict.keys()),\n",
    "                                            cause=\"health\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100 entities\n",
      "352160 non-duplicate tweets for 142 health brands\n"
     ]
    }
   ],
   "source": [
    "healthbrand_twID_dict, healthbrand_twID_twtext = Cause.dedup_tweets(healthbrand_tweets_dict,cause=\"health\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: This function takes some time to run. Please run for once, and save results to file.\n",
      "processed 100000 tweets\n",
      "processed 200000 tweets\n",
      "processed 300000 tweets\n"
     ]
    }
   ],
   "source": [
    "healthbrand_twID_twScore = Cause.score_tweet_by_relevance(healthbrand_twID_twtext,GN_model,health_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "healthbrand_twIDScore = Cause.sort_tweet_by_score(healthbrand_twID_twScore,healthbrand_twID_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cause.select_topn_tweets(filename,healthbrand_twIDScore,healthbrand_twID_twtext,topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 2: feature enginerring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Labeled data for support classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 494 positive instances and 177 negative instances for support classification\n"
     ]
    }
   ],
   "source": [
    "sup_healthbrand_list, sup_healthtweet_list, sup_healthlabel_list = Cause.data_for_sup_clf(brand_health_path,\n",
    "                                                                                          entity='health-brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_health_neg_terms, sup_health_pos_terms = Cause.get_freq_terms(sup_healthtweet_list,sup_healthlabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (non-support):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 74),\n",
       " ('delicious', 42),\n",
       " ('rt', 27),\n",
       " ('eat', 18),\n",
       " ('chocolate', 16),\n",
       " ('milk', 14),\n",
       " ('fresh', 14),\n",
       " ('food', 13),\n",
       " ('amp', 12),\n",
       " ('yummy', 11),\n",
       " ('_NUMBER_', 11),\n",
       " ('make', 9),\n",
       " ('eating', 9),\n",
       " ('products', 9),\n",
       " ('good', 8),\n",
       " ('flavors', 8),\n",
       " ('new', 8),\n",
       " ('fruit', 8),\n",
       " ('us', 7),\n",
       " ('diet', 7)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (non-support):\")\n",
    "sup_health_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in positive class (support):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 315),\n",
       " ('healthy', 182),\n",
       " ('rt', 101),\n",
       " ('delicious', 74),\n",
       " ('_NUMBER_', 67),\n",
       " ('organic', 63),\n",
       " ('foods', 59),\n",
       " ('natural', 53),\n",
       " ('nutritious', 52),\n",
       " ('amp', 51),\n",
       " ('eat', 44),\n",
       " ('_HASHTAG_vegan', 43),\n",
       " ('snack', 40),\n",
       " ('_HASHTAG_organic', 39),\n",
       " ('skin', 39),\n",
       " ('ingredients', 37),\n",
       " ('free', 35),\n",
       " ('food', 33),\n",
       " ('_HASHTAG_healthy', 33),\n",
       " ('vegan', 32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in positive class (support):\")\n",
    "sup_health_pos_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Labeled data for commitment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 238 positive instances and 256 negative instances for commitment classification\n"
     ]
    }
   ],
   "source": [
    "comt_healthbrand_list, comt_healthtweet_list, comt_healthlabel_list = Cause.data_for_commit_clf(brand_health_path,\n",
    "                                                                                          entity='health-brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_health_neg_terms, comt_health_pos_terms = Cause.get_freq_terms(comt_healthtweet_list,comt_healthlabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (low-commitment):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 161),\n",
       " ('healthy', 117),\n",
       " ('foods', 47),\n",
       " ('rt', 46),\n",
       " ('eat', 36),\n",
       " ('_NUMBER_', 32),\n",
       " ('delicious', 31),\n",
       " ('nutritious', 28),\n",
       " ('diet', 24),\n",
       " ('amp', 23),\n",
       " ('skin', 23),\n",
       " ('eating', 22),\n",
       " ('organic', 22),\n",
       " ('snack', 21),\n",
       " ('food', 20),\n",
       " ('_HASHTAG_vegan', 20),\n",
       " ('_HASHTAG_healthy', 17),\n",
       " ('healthier', 17),\n",
       " ('great', 17),\n",
       " ('veggies', 16)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (low-commitment):\")\n",
    "comt_health_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (high-commitment):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 161),\n",
       " ('healthy', 117),\n",
       " ('foods', 47),\n",
       " ('rt', 46),\n",
       " ('eat', 36),\n",
       " ('_NUMBER_', 32),\n",
       " ('delicious', 31),\n",
       " ('nutritious', 28),\n",
       " ('diet', 24),\n",
       " ('amp', 23),\n",
       " ('skin', 23),\n",
       " ('eating', 22),\n",
       " ('organic', 22),\n",
       " ('snack', 21),\n",
       " ('food', 20),\n",
       " ('_HASHTAG_vegan', 20),\n",
       " ('_HASHTAG_healthy', 17),\n",
       " ('healthier', 17),\n",
       " ('great', 17),\n",
       " ('veggies', 16)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (high-commitment):\")\n",
    "comt_health_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 2.1: linguistic cues:<br>\n",
    "2.1.1 Sentiment polarity <br>\n",
    "2.1.2 Pronouns <br>\n",
    "2.1.3 Cause keywords and Context of cause keywords <br>\n",
    "2.1.4 Social interactions <br>\n",
    "2.1.5 Part-Of-Speach tag <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words feature, serve as baseline.\n",
      "(671, 2610)\n",
      "(494, 1983)\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag-of-words feature, serve as baseline.\")\n",
    "sup_BOW_tweet = sup_healthtweet_list\n",
    "sup_bow_vectorizer,sup_BOW_tw_matrix = Cause.construct_feature_matrix(sup_BOW_tweet)\n",
    "print(sup_BOW_tw_matrix.shape)\n",
    "\n",
    "comt_BOW_tweet = comt_healthtweet_list\n",
    "comt_bow_vectorizer, comt_BOW_tw_matrix = Cause.construct_feature_matrix(comt_BOW_tweet)\n",
    "print(comt_BOW_tw_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment polarity feature, for example: [\"It's not organic\"]\n",
      "[\"It's _NEG_ organic\"]\n",
      "(671, 2611)\n",
      "(494, 1984)\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentiment polarity feature, for example: [\\\"It's not organic\\\"]\")\n",
    "print(Cause.mark_polarity([\"It's not organic\"],to_wd=0))\n",
    "sup_BOW_addpola = Cause.mark_polarity(sup_healthtweet_list,to_wd=0)\n",
    "sup_pola_vectorizer,sup_BOW_pola_matrix = Cause.construct_feature_matrix(sup_BOW_addpola)\n",
    "print(sup_BOW_pola_matrix.shape)\n",
    "comt_BOW_addpola = Cause.mark_polarity(comt_healthtweet_list,to_wd=0)\n",
    "comt_pola_vectorizer,comt_BOW_pola_matrix = Cause.construct_feature_matrix(comt_BOW_addpola)\n",
    "print(comt_BOW_pola_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronoun feature, for example:\n",
      "[\"Did you know? I did't but they do first__person second__person third__person\"]\n",
      "(671, 2613)\n",
      "(494, 1986)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pronoun feature, for example:\")    \n",
    "print(Cause.mark_pronouns(['Did you know? I did\\'t but they do'], binary= False))\n",
    "sup_BOW_addPron_tweet = Cause.mark_pronouns(sup_healthtweet_list, binary = False)\n",
    "sup_pron_vectorizer,sup_pron_matrix = Cause.construct_feature_matrix(sup_BOW_addPron_tweet)\n",
    "print(sup_pron_matrix.shape)\n",
    "comt_BOW_addPron_tweet = Cause.mark_pronouns(comt_healthtweet_list, binary = False)\n",
    "comt_pron_vectorizer, comt_pron_matrix = Cause.construct_feature_matrix(comt_BOW_addPron_tweet)\n",
    "print(comt_pron_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eco keywords features, for example:\n",
      "RT @organictvshow: RT if you love Organic   #organic #healthy #wholefoods #nutrition @WholeFoods @Stonyfield @Horizon_Organic @Honest @orga…\n",
      "rt _MENTION_organictvshow rt if you love organic _HASHTAG_organic _HASHTAG_healthy _HASHTAG_wholefoods _HASHTAG_nutrition _MENTION_wholefoods _MENTION_stonyfield _MENTION_horizon_organic _MENTION_honest _MENTION_orga\n",
      "(671, 2623)\n",
      "(494, 1997)\n"
     ]
    }
   ],
   "source": [
    "sup_BOW_addCont_tweet = Cause.mark_context(sup_healthtweet_list,eco_terms)\n",
    "sup_cont_vectorizer,sup_BOW_cont_matrix = Cause.construct_feature_matrix(sup_BOW_addCont_tweet)\n",
    "\n",
    "comt_BOW_addCont_tweet = Cause.mark_context(comt_healthtweet_list,eco_terms)\n",
    "comt_cont_vectorizer, comt_BOW_cont_matrix = Cause.construct_feature_matrix(comt_BOW_addCont_tweet)\n",
    "\n",
    "print(\"Eco keywords features, for example:\")    \n",
    "print(sup_healthtweet_list[80])\n",
    "print(sup_BOW_addCont_tweet[80])\n",
    "print(sup_BOW_cont_matrix.shape)\n",
    "print(comt_BOW_cont_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eco keywords' context features, for example:\n",
      "[\"by walking or taking your bike you won't produce   _URL_   left_context_produce right_context__url_ left_context__url_\"]\n",
      "(671, 2616)\n",
      "(494, 1991)\n"
     ]
    }
   ],
   "source": [
    "print(\"Eco keywords' context features, for example:\")\n",
    "print(Cause.remove_keywords([\"By walking or taking your bike you won't produce greenhouse http://foo.com gas emissions\"],eco_terms))\n",
    "sup_BOW_rmTopic_tweet = Cause.remove_keywords(sup_healthtweet_list, eco_terms)\n",
    "sup_rmtopic_vectorizer,sup_BOW_rmtopic_matrix = Cause.construct_feature_matrix(sup_BOW_rmTopic_tweet)\n",
    "\n",
    "comt_BOW_rmTopic_tweet = Cause.remove_keywords(comt_healthtweet_list, eco_terms)\n",
    "comt_rmtopic_vectorizer, comt_BOW_rmtopic_matrix = Cause.construct_feature_matrix(comt_BOW_rmTopic_tweet)\n",
    "\n",
    "print(sup_BOW_rmtopic_matrix.shape)\n",
    "print(comt_BOW_rmtopic_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfmention features, for example:\n",
      "NEW! IZZE Sparkling Water Beverage. Certified USDA organic and delicious. #organic #SparkleBrightly 💧✨ http://t.co/rkeeMm7lOu\n",
      "NEW! IZZE Sparkling Water Beverage. Certified USDA organic and delicious. #organic #SparkleBrightly 💧✨ http://t.co/rkeeMm7lOu _SELF_\n",
      "_SELF_NEW! _SELF_IZZE _SELF_Sparkling _SELF_Water _SELF_Beverage. _SELF_Certified _SELF_USDA _SELF_organic _SELF_and _SELF_delicious. _SELF_#organic _SELF_#SparkleBrightly _SELF_💧✨ _SELF_http://t.co/rkeeMm7lOu \n"
     ]
    }
   ],
   "source": [
    "sup_BOW_self_tweet_once = Cause.selfmention(sup_healthbrand_list,healthbrand_nameid,sup_healthtweet_list, count=\"once\")\n",
    "sup_BOW_self_tweet_all = Cause.selfmention(sup_healthbrand_list,healthbrand_nameid,sup_healthtweet_list, count=\"all\")\n",
    "\n",
    "comt_BOW_self_tweet_once = Cause.selfmention(comt_healthbrand_list,healthbrand_nameid,comt_healthtweet_list, count=\"once\")\n",
    "comt_BOW_self_tweet_all = Cause.selfmention(comt_healthbrand_list,healthbrand_nameid,comt_healthtweet_list, count=\"all\")\n",
    "\n",
    "print(\"Selfmention features, for example:\")\n",
    "print(sup_healthtweet_list[4])\n",
    "print(sup_BOW_self_tweet_once[4])\n",
    "print(sup_BOW_self_tweet_all[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part-of-speech tagging features, for example:\n",
      "@emilyhalford Because our products contain dairy, they are not vegan!\n",
      "NN IN PRP$ NNS VBP NN PRP VBP RB JJ \n",
      "_MENTION_emilyhalford NN because IN our PRP$ products NNS contain VBP dairy NN they PRP are VBP not RB vegan JJ \n",
      "(671, 2631)\n",
      "(494, 2007)\n"
     ]
    }
   ],
   "source": [
    "sup_BOW_pos_tags,sup_BOW_pos_wdtags = Cause.mark_pos(sup_healthtweet_list)\n",
    "sup_pos_vectorizer,sup_BOW_pos_matrix = Cause.construct_feature_matrix(sup_BOW_pos_wdtags)\n",
    "\n",
    "comt_BOW_pos_tags, comt_BOW_pos_wdtags = Cause.mark_pos(comt_healthtweet_list)\n",
    "comt_pos_vectorizer, comt_BOW_pos_matrix = Cause.construct_feature_matrix(comt_BOW_pos_wdtags)\n",
    "\n",
    "print(\"Part-of-speech tagging features, for example:\")\n",
    "print(sup_healthtweet_list[0])\n",
    "print(sup_BOW_pos_tags[0])\n",
    "print(sup_BOW_pos_wdtags[0])\n",
    "print(sup_BOW_pos_matrix.shape)\n",
    "print(comt_BOW_pos_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 2.1: GrisearchCV to find best parameters for countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__min_df': (1, 3, 5,10,0.3),\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__tokenizer': (None,Cause.tw_tokenize_with_features),\n",
    "    #'vect__max_features': (None, 1000, 2000, 3000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2),(1,3)),  # unigrams or bigrams\n",
    "    #'vect__binary': (True, False),\n",
    "    \n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "\n",
    "    'lr__penalty': ('l2', 'l1'),\n",
    "    'lr__class_weight': (\"balanced\",None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'lr']\n",
      "parameters:\n",
      "{'lr__class_weight': ('balanced', None),\n",
      " 'lr__penalty': ('l2', 'l1'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__min_df': (1, 3, 5, 10, 0.3),\n",
      " 'vect__tokenizer': (None,\n",
      "                     <function tw_tokenize_with_features at 0x7fa4fc1537b8>)}\n",
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 6.144s\n",
      "\n",
      "Best score: 0.901\n",
      "Best parameters set:\n",
      "\tlr__class_weight: None\n",
      "\tlr__penalty: 'l2'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__min_df: 1\n",
      "\tvect__tokenizer: None\n"
     ]
    }
   ],
   "source": [
    "Cause.do_grid_search(pipeline,parameters,data = sup_healthtweet_list, label = sup_healthlabel_list, score='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'lr']\n",
      "parameters:\n",
      "{'lr__class_weight': ('balanced', None),\n",
      " 'lr__penalty': ('l2', 'l1'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__min_df': (1, 3, 5, 10, 0.3),\n",
      " 'vect__tokenizer': (None,\n",
      "                     <function tw_tokenize_with_features at 0x7fa4fc1537b8>)}\n",
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:    1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.099s\n",
      "\n",
      "Best score: 0.700\n",
      "Best parameters set:\n",
      "\tlr__class_weight: 'balanced'\n",
      "\tlr__penalty: 'l2'\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__min_df: 1\n",
      "\tvect__tokenizer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:    4.0s finished\n"
     ]
    }
   ],
   "source": [
    "Cause.do_grid_search(pipeline,parameters,data = comt_healthtweet_list, label = comt_healthlabel_list, score='f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 2.2: word2vec features: <br>\n",
    "2.2.1 Tweet vector and tweet cause relevance score <br>\n",
    "2.2.2 Top-n(n=3,5) words, top-n words' vector and top-n words' cause relevance scores <br>\n",
    "2.2.3 Cause keywords(sim>=0.30), number of cause keywords, cause keywords' vector, cause words' relevance scores <br>\n",
    "2.2.4 Context words (window = 1), context vector, context words' contribution scores <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet vector:\n",
      "(671, 300)\n",
      "(494, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet vector:\")\n",
    "sup_W2V_tw_vector = Cause.construct_tw_vector(sup_healthtweet_list,GN_model)\n",
    "print(sup_W2V_tw_vector.shape)\n",
    "comt_W2V_tw_vector = Cause.construct_tw_vector(comt_healthtweet_list,GN_model)\n",
    "print(comt_W2V_tw_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet cause-relevance score:\n",
      "(671, 1)\n",
      "(494, 1)\n",
      "@ASButtland chips (sugar, chocolate liquor, cocoa butter, soy lecithin, natural flavor) I'd recommend checking the package as ingredients...\n",
      "[ 0.634]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tweet cause-relevance score:\")\n",
    "sup_W2V_tw_score = Cause.calculate_tw_w2v_score(sup_healthtweet_list,GN_model,health_keywords)\n",
    "print((sup_W2V_tw_score.shape))\n",
    "comt_W2V_tw_score = Cause.calculate_tw_w2v_score(comt_healthtweet_list,GN_model,health_keywords)\n",
    "print((comt_W2V_tw_score.shape))\n",
    "print(sup_healthtweet_list[10])\n",
    "print(sup_W2V_tw_score[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rand words by cause-relevance:\n",
      "For example:\n",
      "@emilyhalford Because our products contain dairy, they are not vegan!\n",
      "[('vegan', '0.694'), ('dairy', '0.467'), ('products', '0.321'), ('contain', '0.169')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Rand words by cause-relevance:\\nFor example:\")\n",
    "sup_tweet_rankedwd_list = Cause.rank_match_words(sup_healthtweet_list, GN_model, health_keywords)\n",
    "comt_tweet_rankedwd_list = Cause.rank_match_words(comt_healthtweet_list, GN_model, health_keywords)\n",
    "print(sup_healthtweet_list[0])\n",
    "print(sup_tweet_rankedwd_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-n words in each tweet:\n",
      "For example:\n",
      "@emilyhalford Because our products contain dairy, they are not vegan!\n",
      "[('vegan', '0.694'), ('dairy', '0.467'), ('products', '0.321'), ('contain', '0.169')]\n",
      "vegan dairy products \n",
      "[ 0.694  0.467  0.321]\n",
      "(671, 419)\n",
      "(494, 274)\n"
     ]
    }
   ],
   "source": [
    "print(\"Top-n words in each tweet:\\nFor example:\")\n",
    "sup_W2V_topnwd, sup_W2V_topnwd_scores = Cause.get_topn_words(sup_tweet_rankedwd_list,n=3)\n",
    "sup_topn_vectorizer,sup_W2V_topnwd_matrix = Cause.construct_feature_matrix(sup_W2V_topnwd)\n",
    "print(sup_healthtweet_list[0])\n",
    "print(sup_tweet_rankedwd_list[0])\n",
    "print(sup_W2V_topnwd[0])\n",
    "print(sup_W2V_topnwd_scores[0])\n",
    "\n",
    "print(sup_W2V_topnwd_matrix.shape)\n",
    "\n",
    "comt_W2V_topnwd, comt_W2V_topnwd_scores = Cause.get_topn_words(comt_tweet_rankedwd_list,n=3)\n",
    "comt_topn_vectorizer, comt_W2V_topnwd_matrix = Cause.construct_feature_matrix(comt_W2V_topnwd)\n",
    "print(comt_W2V_topnwd_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each tweet is represented by mean of top-n words' vectors:\n",
      "(671, 300)\n",
      "(494, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Each tweet is represented by mean of top-n words' vectors:\")\n",
    "sup_W2V_topnwd_vectors = Cause.get_topn_vectors(sup_tweet_rankedwd_list,GN_model,n=3)\n",
    "print(sup_W2V_topnwd_vectors.shape)\n",
    "comt_W2V_topnwd_vectors = Cause.get_topn_vectors(comt_tweet_rankedwd_list,GN_model,n=3)\n",
    "print(comt_W2V_topnwd_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with cause-relevance score >= 0.3 serve as cause keywords:\n",
      "Organized as:[relevance-score, leftword_contribution, rightword_contribution].\n",
      "For example:\n",
      "@ASButtland chips (sugar, chocolate liquor, cocoa butter, soy lecithin, natural flavor) I'd recommend checking the package as ingredients...\n",
      "sugar chocolate cocoa butter soy lecithin natural flavor ingredients \n",
      "{'chocolate': [0.444, 0.534, 0.278], 'lecithin': [0.459, 0.502, 0.228], 'cocoa': [0.319, 0.186, 0.249], 'natural': [0.478, 0.228, 0.144], 'sugar': [0.396, 0.131, 0.534], 'ingredients': [0.481, 0.057, 0.0], 'soy': [0.501, 0.284, 0.502], 'flavor': [0.381, 0.144, -0.039], 'butter': [0.3, 0.249, 0.284]}\n",
      "(671, 455)\n",
      "(494, 378)\n"
     ]
    }
   ],
   "source": [
    "print(\"Words with cause-relevance score >= 0.3 serve as cause keywords:\\nOrganized as:[relevance-score, leftword_contribution, rightword_contribution].\\nFor example:\")\n",
    "sup_W2V_topicwd_list,sup_tweet_topicwd_tp_list = Cause.get_topic_words(sup_healthtweet_list,GN_model,health_keywords,threshold = 0.30)\n",
    "sup_topic_vectorizer,sup_Topicwd_matrix = Cause.construct_feature_matrix(sup_W2V_topicwd_list)\n",
    "print(sup_healthtweet_list[10])\n",
    "print(sup_W2V_topicwd_list[10])\n",
    "print(sup_tweet_topicwd_tp_list[10])\n",
    "print(sup_Topicwd_matrix.shape)\n",
    "\n",
    "comt_W2V_topicwd_list, comt_tweet_topicwd_tp_list = Cause.get_topic_words(comt_healthtweet_list,GN_model,health_keywords,threshold = 0.30)\n",
    "comt_topic_vectorizer, comt_Topicwd_matrix = Cause.construct_feature_matrix(comt_W2V_topicwd_list)\n",
    "print(comt_Topicwd_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each tweet's keywords' vector represented by mean of cause-relevant words' vectors:\n",
      "(671, 300)\n",
      "(494, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Each tweet's keywords' vector represented by mean of cause-relevant words' vectors:\")\n",
    "sup_W2V_topicwd_vectors = Cause.get_topicwd_vec(GN_model,sup_tweet_topicwd_tp_list)\n",
    "print(sup_W2V_topicwd_vectors.shape)\n",
    "\n",
    "comt_W2V_topicwd_vectors = Cause.get_topicwd_vec(GN_model,comt_tweet_topicwd_tp_list)\n",
    "print(comt_W2V_topicwd_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get number of cause keywords, keywords' cause-relevance scores, keywords' left word contribution scores, keywords' right word contribution scores.\n"
     ]
    }
   ],
   "source": [
    "print(\"Get number of cause keywords, keywords' cause-relevance scores, keywords' left word contribution scores, keywords' right word contribution scores.\")\n",
    "sup_W2V_topicwd_ct,sup_W2V_topicwd_score,sup_W2V_topicwd_leftcontri,sup_W2V_topicwd_rightcontri = Cause.sep_topic_features(sup_tweet_topicwd_tp_list)\n",
    "comt_W2V_topicwd_ct,comt_W2V_topicwd_score,comt_W2V_topicwd_leftcontri,comt_W2V_topicwd_rightcontri = Cause.sep_topic_features(comt_tweet_topicwd_tp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cause keywords in each tweet, for example:\n",
      "So good you’ll want to pop them in your fruit basket -- fresh new scents from Ulta Beauty Collection! https://t.co/5mZ9zP8wCp\n",
      "{'beauty': [0.302, 0.147, 0.147], 'fruit': [0.376, 0.098, 0.149], 'fresh': [0.366, 0.134, 0.445], 'good': [0.308, 0.301, 0.344]}\n",
      "[4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cause keywords in each tweet, for example:\")\n",
    "print(sup_healthtweet_list[150])\n",
    "print(sup_tweet_topicwd_tp_list[150])\n",
    "print(sup_W2V_topicwd_ct[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cause keywords' cause-relevance scores, for example:\n",
      "Always end up eating too much when I have yummy Chinese food!! #nowcantmove Mx\n",
      "{'yummy': [0.552, 0.036, 0.248], 'food': [0.508, 0.22, 0.0], 'eating': [0.54, 0.045, 0.233]}\n",
      "[0.552, 0.508, 0.54]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cause keywords' cause-relevance scores, for example:\")\n",
    "print(sup_healthtweet_list[1])\n",
    "print(sup_tweet_topicwd_tp_list[1])\n",
    "print(sup_W2V_topicwd_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of keywords' cause-relevance scores, for example:\n",
      "[0.552, 0.508, 0.54]\n",
      "[ 1.6]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of keywords' cause-relevance scores, for example:\")\n",
    "sup_W2V_topicwd_sum = Cause.get_topicwd_score_sum(sup_W2V_topicwd_score)\n",
    "comt_W2V_topicwd_sum = Cause.get_topicwd_score_sum(comt_W2V_topicwd_score)\n",
    "print(sup_W2V_topicwd_score[1])\n",
    "print(sup_W2V_topicwd_sum[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywors' left context words' contribution scores:\n",
      "(671,)\n"
     ]
    }
   ],
   "source": [
    "print(\"keywors' left context words' contribution scores:\")\n",
    "print(sup_W2V_topicwd_leftcontri.shape)# not fixed elementes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of keywords' context words' contribution scores:\n",
      "(671, 3)\n",
      "[ 0.623  0.945  0.953]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum of keywords' context words' contribution scores:\")\n",
    "sup_W2V_contri_score = Cause.get_contri_sum(sup_tweet_topicwd_tp_list)\n",
    "comt_W2V_contri_score = Cause.get_contri_sum(comt_tweet_topicwd_tp_list)\n",
    "print(sup_W2V_contri_score.shape)\n",
    "print(sup_W2V_contri_score[150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 3: train and evaluate support classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Use logistic regression as a basic classifier.\n",
    "sup_lr = LogisticRegression(solver = 'lbfgs',multi_class ='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 3.1: Evaluating linguistic features, word embedding features and combination of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bag-of-words\n",
    "sup_bow_vectorizer,sup_BOW_tw_matrix = Cause.construct_feature_matrix(sup_BOW_tweet)\n",
    "\n",
    "#bag-of-words + polarity\n",
    "sup_bowneg_vectorizer,sup_BOW_neg_matrix = Cause.construct_feature_matrix(sup_BOW_addpola)\n",
    "\n",
    "#bag-of-words + pronoun\n",
    "sup_pron_vectorizer,sup_BOW_pron_matrix = Cause.construct_feature_matrix(sup_BOW_addPron_tweet)\n",
    "\n",
    "#bag-of-words + keywords' context\n",
    "sup_cont_vectorizer,sup_BOW_cont_matrix = Cause.construct_feature_matrix(sup_BOW_addCont_tweet)\n",
    "\n",
    "#bag-of-words + remove causekeywords\n",
    "sup_rmeco_vectorizer,sup_BOW_rmTopic_matrix = Cause.construct_feature_matrix(sup_BOW_rmTopic_tweet)\n",
    "\n",
    "#bag-of-words + self_mention\n",
    "sup_BOW_self_tweet_once = Cause.selfmention(sup_healthbrand_list,healthbrand_nameid,sup_healthtweet_list, count=\"once\")\n",
    "sup_mention1_vectorizer,sup_BOW_mentionOnce_matrix = Cause.construct_feature_matrix(sup_BOW_self_tweet_once)\n",
    "#pickle.dump(mention1_vectorizer, open(PATH+\"Tweet_Health_self.vectorizer\", 'wb'))\n",
    "\n",
    "#bag-of-words + self_mention to all words\n",
    "sup_BOW_self_tweet_all = Cause.selfmention(sup_healthbrand_list,healthbrand_nameid,sup_healthtweet_list, count=\"all\")\n",
    "sup_mentionAll_vectorizer,sup_BOW_mentionAll_matrix = Cause.construct_feature_matrix(sup_BOW_self_tweet_all)\n",
    "\n",
    "#bag-of-words + self_mention + pronoun\n",
    "sup_BOW_self_pron_tweet = Cause.selfmention(sup_healthbrand_list,healthbrand_nameid,sup_BOW_addPron_tweet, count=\"once\")\n",
    "sup_mention1pron_vectorizer,sup_BOW_mentionOncePron_matrix = Cause.construct_feature_matrix(sup_BOW_self_pron_tweet)\n",
    "\n",
    "#bag-of-words + self_mention to all words + pronoun\n",
    "sup_BOW_selfall_pron_tweet = Cause.selfmention(sup_healthbrand_list,healthbrand_nameid,sup_BOW_addPron_tweet, count=\"all\")\n",
    "sup_mentionAllpron_vectorizer,sup_BOW_mentionAllPron_matrix = Cause.construct_feature_matrix(sup_BOW_selfall_pron_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_Lingu_feature_names = [\"sup_BOW_tw_matrix\",\"sup_BOW_neg_matrix\",\"sup_BOW_pron_matrix\",\"sup_BOW_cont_matrix\", \n",
    "                           \"sup_BOW_rmTopic_matrix\",\"sup_BOW_mentionOnce_matrix\",\"sup_BOW_mentionAll_matrix\", \n",
    "                           \"sup_BOW_mentionOncePron_matrix\", \"sup_BOW_mentionAllPron_matrix\"]\n",
    "\n",
    "sup_Lingu_features = [sup_BOW_tw_matrix, sup_BOW_neg_matrix, sup_BOW_pron_matrix, sup_BOW_cont_matrix, sup_BOW_rmTopic_matrix,\n",
    "        sup_BOW_mentionOnce_matrix, sup_BOW_mentionAll_matrix, sup_BOW_mentionOncePron_matrix, sup_BOW_mentionAllPron_matrix]\n",
    "sup_Lingu_feature_dict = {}\n",
    "for i in range(len(sup_Lingu_feature_names)):\n",
    "    sup_Lingu_feature_dict[sup_Lingu_feature_names[i]] = sup_Lingu_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup_BOW_tw_matrix\t0.909\n",
      "sup_BOW_mentionOnce_matrix\t0.908\n",
      "sup_BOW_neg_matrix\t0.907\n",
      "sup_BOW_mentionOncePron_matrix\t0.907\n",
      "sup_BOW_cont_matrix\t0.906\n",
      "sup_BOW_rmTopic_matrix\t0.906\n",
      "sup_BOW_pron_matrix\t0.906\n",
      "sup_BOW_mentionAll_matrix\t0.889\n",
      "sup_BOW_mentionAllPron_matrix\t0.886\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_bow_feature(sup_Lingu_feature_dict,sup_healthlabel_list,sup_lr,mycv,score_func='f1')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_W2V_feature_names = [\"sup_W2V_tw_score\", \"sup_W2V_tw_vector\", \"sup_W2V_topnwd_matrix\", \"sup_W2V_topnwd_scores\", \n",
    "                     \"sup_W2V_topnwd_vectors\", \"sup_Topicwd_matrix\", \"sup_W2V_topicwd_ct\", \"sup_W2V_topicwd_sum\", \n",
    "                     \"sup_W2V_contri_score\", \"sup_W2V_topicwd_vectors\"]\n",
    "\n",
    "sup_W2V_features = [sup_W2V_tw_score, sup_W2V_tw_vector, sup_W2V_topnwd_matrix, sup_W2V_topnwd_scores, sup_W2V_topnwd_vectors, \n",
    "                sup_Topicwd_matrix, sup_W2V_topicwd_ct, sup_W2V_topicwd_sum, sup_W2V_contri_score, sup_W2V_topicwd_vectors]\n",
    "sup_W2V_feature_dict = {}\n",
    "for i in range(len(sup_W2V_feature_names)):\n",
    "    sup_W2V_feature_dict[sup_W2V_feature_names[i]] = sup_W2V_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup_W2V_topicwd_vectors\t0.930\n",
      "sup_W2V_topnwd_vectors\t0.928\n",
      "sup_W2V_tw_vector\t0.927\n",
      "sup_Topicwd_matrix\t0.920\n",
      "sup_W2V_topnwd_matrix\t0.915\n",
      "sup_W2V_topnwd_scores\t0.886\n",
      "sup_W2V_topicwd_sum\t0.876\n",
      "sup_W2V_contri_score\t0.874\n",
      "sup_W2V_tw_score\t0.869\n",
      "sup_W2V_topicwd_ct\t0.864\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_w2v_feature(sup_W2V_feature_dict,sup_ecolabel_list,sup_lr,mycv,score_func='f1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_COM_feature_names = [\"sup_BOW_tw_matrix\",\"sup_BOW_mentionOnce_matrix\",\"sup_BOW_mentionOncePron_matrix\",\"sup_BOW_cont_matrix\",\n",
    "                     \"sup_W2V_tw_vector\", \"sup_W2V_topnwd_matrix\", \"sup_W2V_topnwd_vectors\", \"sup_W2V_topicwd_vectors\"]\n",
    "\n",
    "sup_COM_features = [sup_BOW_tw_matrix,sup_BOW_mentionOnce_matrix,sup_BOW_mentionOncePron_matrix,sup_BOW_cont_matrix,\n",
    "                sup_W2V_tw_vector, sup_W2V_topnwd_matrix, sup_W2V_topnwd_vectors, sup_W2V_topicwd_vectors]\n",
    "sup_COM_feature_dict = {}\n",
    "for i in range(len(sup_COM_feature_names)):\n",
    "    sup_COM_feature_dict[sup_COM_feature_names[i]] = sup_COM_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine 1 features.\n",
      "Combine 2 features.\n",
      "Combine 3 features.\n",
      "Combine 4 features.\n",
      "Combine 5 features.\n",
      "sup_BOW_tw_matrix + sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.936\n",
      "sup_BOW_mentionOnce_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.935\n",
      "sup_BOW_mentionAllPron_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.935\n",
      "sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.935\n",
      "sup_BOW_tw_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.935\n",
      "sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.935\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.935\n",
      "sup_BOW_mentionOnce_matrix + sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.934\n",
      "sup_BOW_mentionAllPron_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.933\n",
      "sup_BOW_tw_matrix + sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors\t0.933\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_comb_feature(sup_COM_feature_names,sup_COM_feature_dict,sup_ecolabel_list,sup_lr,mycv,score_func='f1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.918\n",
      "recall:0.954\n",
      "f1:0.936\n"
     ]
    }
   ],
   "source": [
    "com_Best_feature_names=[\"sup_BOW_tw_matrix\",\"sup_BOW_cont_matrix\",\"sup_W2V_tw_vector\", \"sup_W2V_topnwd_vectors\"]\n",
    "com_Best_feature_list = [sup_BOW_tw_matrix,sup_BOW_cont_matrix,sup_W2V_tw_vector, sup_W2V_topnwd_vectors]\n",
    "com_Best_feature_dict = {}\n",
    "for i in range(len(com_Best_feature_names)):\n",
    "    com_Best_feature_dict[com_Best_feature_names[i]] = com_Best_feature_names[i]\n",
    "   \n",
    "com_Best_feature = com_Best_feature_list[0]\n",
    "for feature in com_Best_feature_list[1:]:\n",
    "    com_Best_feature = np.hstack((com_Best_feature,feature))\n",
    "\n",
    "print(\"precision:%.3f\" % np.mean(cross_val_score(sup_lr, com_Best_feature, sup_healthlabel_list,cv=mycv,scoring='precision')))\n",
    "print(\"recall:%.3f\" % np.mean(cross_val_score(sup_lr, com_Best_feature, sup_healthlabel_list,cv=mycv,scoring='recall')))\n",
    "print(\"f1:%.3f\" % np.mean(cross_val_score(sup_lr, com_Best_feature, sup_healthlabel_list,cv=mycv,scoring='f1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 3.2: Evaluate different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(multi_class ='ovr',penalty='l2',class_weight=\"balanced\")\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "nn = MLPClassifier(solver='lbfgs',alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923\tLogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.811\tGaussianNB(priors=None)\n",
      "0.893\tRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "0.923\tMLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_classifier(com_Best_feature,sup_healthlabel_list,mycv,score_func='f1',classifier_list = [lr,gnb,rf,nn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 3.3 Analyze terms that have high coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_BOW_lr = LogisticRegression(penalty=\"l2\",class_weight=\"balanced\")\n",
    "sup_BOW_lr.fit(sup_BOW_mentionOnce_matrix, sup_healthlabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 positive coefficient words:\n",
      "             healthy\t3.136\n",
      "          nutritious\t2.664\n",
      "             organic\t1.817\n",
      "    _HASHTAG_healthy\t1.785\n",
      "    _HASHTAG_organic\t1.606\n",
      "      _HASHTAG_vegan\t1.594\n",
      "           healthier\t1.588\n",
      "             natural\t1.400\n",
      "           wholesome\t1.276\n",
      "                free\t0.933\n",
      "               foods\t0.848\n",
      "               vegan\t0.834\n",
      "            calories\t0.831\n",
      "              fruits\t0.786\n",
      "                help\t0.744\n",
      "               great\t0.732\n",
      "          vegetarian\t0.725\n",
      "                know\t0.682\n",
      "             veggies\t0.664\n",
      "                skin\t0.653\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 positive coefficient words:\")\n",
    "for i in np.argsort(sup_BOW_lr.coef_[0])[::-1][:20]:\n",
    "    print('%20s\\t%.3f' % (sup_mention1_vectorizer.get_feature_names()[i], sup_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 negative coefficient words:\n",
      "               water\t-1.198\n",
      "   _HASHTAG_cleanser\t-1.033\n",
      "              flavor\t-0.883\n",
      "                  hi\t-0.818\n",
      "           chocolate\t-0.732\n",
      "             animals\t-0.725\n",
      "              fruity\t-0.717\n",
      "                 pop\t-0.656\n",
      "             looking\t-0.617\n",
      "             contain\t-0.609\n",
      " _MENTION_doratahair\t-0.601\n",
      "               sleek\t-0.601\n",
      "  _HASHTAG_sleekchic\t-0.601\n",
      "                time\t-0.582\n",
      "              cereal\t-0.579\n",
      "              cheese\t-0.572\n",
      "               enjoy\t-0.572\n",
      "               isn't\t-0.556\n",
      "          ingredient\t-0.553\n",
      "                life\t-0.538\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 negative coefficient words:\")\n",
    "for i in np.argsort(sup_BOW_lr.coef_[0])[::1][:20]:\n",
    "    print('%20s\\t%.3f' % (sup_mention1_vectorizer.get_feature_names()[i], sup_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** Section 4: train and evaluate commitment classifier with manually labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Use logistic regression as a basic classifier.\n",
    "comt_lr = LogisticRegression(solver = 'lbfgs',multi_class ='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 4.1: Evaluating linguistic features, word embedding features and combination of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bag-of-words\n",
    "comt_bow_vectorizer,comt_BOW_tw_matrix = Cause.construct_feature_matrix(comt_BOW_tweet)\n",
    "\n",
    "#bag-of-words + polarity\n",
    "comt_bowneg_vectorizer,comt_BOW_neg_matrix = Cause.construct_feature_matrix(comt_BOW_addpola)\n",
    "\n",
    "#bag-of-words + pronoun\n",
    "comt_pron_vectorizer,comt_BOW_pron_matrix = Cause.construct_feature_matrix(comt_BOW_addPron_tweet)\n",
    "\n",
    "#bag-of-words + keywords' context\n",
    "comt_cont_vectorizer,comt_BOW_cont_matrix = Cause.construct_feature_matrix(comt_BOW_addCont_tweet)\n",
    "\n",
    "#bag-of-words + remove causekeywords\n",
    "comt_rmeco_vectorizer,comt_BOW_rmTopic_matrix = Cause.construct_feature_matrix(comt_BOW_rmTopic_tweet)\n",
    "\n",
    "#bag-of-words + self_mention\n",
    "comt_BOW_self_tweet_once = Cause.selfmention(comt_healthbrand_list,healthbrand_nameid,comt_healthtweet_list, count=\"once\")\n",
    "comt_mention1_vectorizer,comt_BOW_mentionOnce_matrix = Cause.construct_feature_matrix(comt_BOW_self_tweet_once)\n",
    "#pickle.dump(mention1_vectorizer, open(PATH+\"Tweet_Health_self.vectorizer\", 'wb'))\n",
    "\n",
    "#bag-of-words + self_mention to all words\n",
    "comt_BOW_self_tweet_all = Cause.selfmention(comt_healthbrand_list,healthbrand_nameid,comt_healthtweet_list, count=\"all\")\n",
    "comt_mentionAll_vectorizer,comt_BOW_mentionAll_matrix = Cause.construct_feature_matrix(comt_BOW_self_tweet_all)\n",
    "\n",
    "#bag-of-words + self_mention + pronoun\n",
    "comt_BOW_self_pron_tweet = Cause.selfmention(comt_healthbrand_list,healthbrand_nameid,comt_BOW_addPron_tweet, count=\"once\")\n",
    "comt_mention1pron_vectorizer,comt_BOW_mentionOncePron_matrix = Cause.construct_feature_matrix(comt_BOW_self_pron_tweet)\n",
    "\n",
    "#bag-of-words + self_mention to all words + pronoun\n",
    "comt_BOW_selfall_pron_tweet = Cause.selfmention(comt_healthbrand_list,healthbrand_nameid,comt_BOW_addPron_tweet, count=\"all\")\n",
    "comt_mentionAllpron_vectorizer,comt_BOW_mentionAllPron_matrix = Cause.construct_feature_matrix(comt_BOW_selfall_pron_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_Lingu_feature_names = [\"comt_BOW_tw_matrix\",\"comt_BOW_neg_matrix\",\"comt_BOW_pron_matrix\",\"comt_BOW_cont_matrix\", \n",
    "                           \"comt_BOW_rmTopic_matrix\",\"comt_BOW_mentionOnce_matrix\",\"comt_BOW_mentionAll_matrix\", \n",
    "                           \"comt_BOW_mentionOncePron_matrix\", \"comt_BOW_mentionAllPron_matrix\"]\n",
    "\n",
    "comt_Lingu_features = [comt_BOW_tw_matrix, comt_BOW_neg_matrix, comt_BOW_pron_matrix, comt_BOW_cont_matrix, comt_BOW_rmTopic_matrix,\n",
    "        comt_BOW_mentionOnce_matrix, comt_BOW_mentionAll_matrix, comt_BOW_mentionOncePron_matrix, comt_BOW_mentionAllPron_matrix]\n",
    "comt_Lingu_feature_dict = {}\n",
    "for i in range(len(comt_Lingu_feature_names)):\n",
    "    comt_Lingu_feature_dict[comt_Lingu_feature_names[i]] = comt_Lingu_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comt_BOW_mentionAllPron_matrix\t0.742\n",
      "comt_BOW_mentionAll_matrix\t0.732\n",
      "comt_BOW_mentionOncePron_matrix\t0.723\n",
      "comt_BOW_mentionOnce_matrix\t0.712\n",
      "comt_BOW_neg_matrix\t0.683\n",
      "comt_BOW_tw_matrix\t0.680\n",
      "comt_BOW_rmTopic_matrix\t0.679\n",
      "comt_BOW_cont_matrix\t0.679\n",
      "comt_BOW_pron_matrix\t0.675\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_bow_feature(comt_Lingu_feature_dict,comt_healthlabel_list,comt_lr,mycv,score_func='f1')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_W2V_feature_names = [\"comt_W2V_tw_vector\", \"comt_W2V_topnwd_matrix\", \"comt_W2V_topnwd_scores\", \n",
    "                     \"comt_W2V_topnwd_vectors\", \"comt_Topicwd_matrix\", \"comt_W2V_topicwd_ct\", \n",
    "                     \"comt_W2V_contri_score\", \"comt_W2V_topicwd_vectors\"]\n",
    "\n",
    "comt_W2V_features = [comt_W2V_tw_vector, comt_W2V_topnwd_matrix, comt_W2V_topnwd_scores, comt_W2V_topnwd_vectors, \n",
    "                comt_Topicwd_matrix, comt_W2V_topicwd_ct, comt_W2V_contri_score, comt_W2V_topicwd_vectors]\n",
    "comt_W2V_feature_dict = {}\n",
    "for i in range(len(comt_W2V_feature_names)):\n",
    "    comt_W2V_feature_dict[comt_W2V_feature_names[i]] = comt_W2V_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comt_W2V_tw_vector\t0.697\n",
      "comt_W2V_topicwd_vectors\t0.677\n",
      "comt_Topicwd_matrix\t0.671\n",
      "comt_W2V_topnwd_vectors\t0.656\n",
      "comt_W2V_topnwd_matrix\t0.640\n",
      "comt_W2V_topnwd_scores\t0.416\n",
      "comt_W2V_topicwd_ct\t0.399\n",
      "comt_W2V_contri_score\t0.322\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_w2v_feature(comt_W2V_feature_dict,comt_healthlabel_list,comt_lr,mycv,score_func='f1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_COM_feature_names = [\"comt_BOW_tw_matrix\",\"comt_BOW_mentionOnce_matrix\",\"comt_BOW_mentionAll_matrix\",\"comt_BOW_mentionAllPron_matrix\",\"comt_BOW_cont_matrix\",\n",
    "                     \"comt_W2V_tw_vector\", \"comt_W2V_topnwd_matrix\", \"comt_W2V_topnwd_vectors\", \"comt_W2V_topicwd_vectors\"]\n",
    "\n",
    "comt_COM_features = [comt_BOW_tw_matrix,comt_BOW_mentionOnce_matrix,comt_BOW_mentionAll_matrix,comt_BOW_mentionAllPron_matrix,comt_BOW_cont_matrix,\n",
    "                comt_W2V_tw_vector, comt_W2V_topnwd_matrix, comt_W2V_topnwd_vectors, comt_W2V_topicwd_vectors]\n",
    "comt_COM_feature_dict = {}\n",
    "for i in range(len(comt_COM_feature_names)):\n",
    "    comt_COM_feature_dict[comt_COM_feature_names[i]] = comt_COM_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine 1 features.\n",
      "Combine 2 features.\n",
      "Combine 3 features.\n",
      "Combine 4 features.\n",
      "Combine 5 features.\n",
      "comt_BOW_mentionAll_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_cont_matrix + comt_W2V_topnwd_vectors + comt_W2V_topicwd_vectors\t0.750\n",
      "comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_topnwd_matrix + comt_W2V_topnwd_vectors + comt_W2V_topicwd_vectors\t0.750\n",
      "comt_BOW_mentionAll_matrix + comt_BOW_mentionAllPron_matrix\t0.749\n",
      "comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix + comt_W2V_topicwd_vectors\t0.749\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix + comt_W2V_topicwd_vectors\t0.749\n",
      "comt_BOW_mentionAll_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector\t0.749\n",
      "comt_BOW_mentionAll_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_cont_matrix + comt_W2V_topnwd_vectors\t0.747\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionAll_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_topnwd_vectors + comt_W2V_topicwd_vectors\t0.747\n",
      "comt_BOW_mentionAllPron_matrix + comt_W2V_topicwd_vectors\t0.747\n",
      "comt_BOW_mentionAll_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_topnwd_matrix + comt_W2V_topnwd_vectors + comt_W2V_topicwd_vectors\t0.747\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_comb_feature(comt_COM_feature_names,comt_COM_feature_dict,comt_healthlabel_list,comt_lr,mycv,score_func='f1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.760\n",
      "recall:0.739\n",
      "f1:0.747\n"
     ]
    }
   ],
   "source": [
    "com_Best_feature_names2=[\"comt_BOW_mentionOnce_matrix\", \"comt_BOW_mentionAllPron_matrix\",\"comt_W2V_topnwd_matrix\",\n",
    "                         \"comt_W2V_topnwd_vectors\",\"comt_W2V_topicwd_vectors\"]\n",
    "com_Best_feature_list2 = [comt_BOW_mentionOnce_matrix, comt_BOW_mentionAllPron_matrix,comt_W2V_topnwd_matrix,\n",
    "                         comt_W2V_topnwd_vectors,comt_W2V_topicwd_vectors]\n",
    "com_Best_feature_dict2 = {}\n",
    "for i in range(len(com_Best_feature_names2)):\n",
    "    com_Best_feature_dict2[com_Best_feature_names2[i]] = com_Best_feature_names2[i]\n",
    "   \n",
    "comt_com_Best_feature2 = com_Best_feature_list2[0]\n",
    "for feature in com_Best_feature_list2[1:]:\n",
    "    comt_com_Best_feature2 = np.hstack((comt_com_Best_feature2,feature))\n",
    "\n",
    "print(\"precision:%.3f\" % np.mean(cross_val_score(comt_lr, comt_com_Best_feature2, comt_healthlabel_list,cv=mycv,scoring='precision')))\n",
    "print(\"recall:%.3f\" % np.mean(cross_val_score(comt_lr, comt_com_Best_feature2, comt_healthlabel_list,cv=mycv,scoring='recall')))\n",
    "print(\"f1:%.3f\" % np.mean(cross_val_score(comt_lr, comt_com_Best_feature2, comt_healthlabel_list,cv=mycv,scoring='f1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 4.2: Evaluate different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(multi_class ='ovr',penalty='l2',class_weight=\"balanced\")\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "nn = MLPClassifier(solver='lbfgs',alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747\tLogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.649\tGaussianNB(priors=None)\n",
      "0.597\tRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "0.731\tMLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_classifier(comt_com_Best_feature2,comt_healthlabel_list,mycv,score_func='f1',classifier_list = [lr,gnb,rf,nn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 4.3: Analyze terms that have high coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comt_BOW_lr = LogisticRegression(penalty=\"l2\",class_weight=\"balanced\")\n",
    "comt_BOW_lr.fit(comt_BOW_mentionAll_matrix, comt_healthlabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 positive coefficient words:\n",
      "            _self_rt\t1.331\n",
      "             flavors\t1.045\n",
      "         ingredients\t1.029\n",
      "          _self_with\t1.006\n",
      "             natural\t0.988\n",
      "     _HASHTAG_nongmo\t0.981\n",
      "               vegan\t0.924\n",
      "              dishes\t0.863\n",
      "                 new\t0.847\n",
      "       _self_organic\t0.829\n",
      "               bread\t0.799\n",
      "                 add\t0.788\n",
      "                just\t0.765\n",
      "           certified\t0.745\n",
      "                 tea\t0.726\n",
      "                 gmo\t0.696\n",
      "               sweet\t0.651\n",
      "                   2\t0.647\n",
      "_self__HASHTAG_vegan\t0.632\n",
      "       _self_natural\t0.609\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 positive coefficient words:\")\n",
    "for i in np.argsort(comt_BOW_lr.coef_[0])[::-1][:20]:\n",
    "    print('%20s\\t%.3f' % (comt_mentionAll_vectorizer.get_feature_names()[i], comt_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 negative coefficient words:\n",
      "                  rt\t-1.363\n",
      "              eating\t-1.064\n",
      "    _HASHTAG_healthy\t-1.019\n",
      "               foods\t-1.005\n",
      "                 eat\t-0.875\n",
      "                hair\t-0.865\n",
      "                best\t-0.834\n",
      "               right\t-0.797\n",
      "           important\t-0.712\n",
      "               think\t-0.704\n",
      "                 raw\t-0.637\n",
      "              recipe\t-0.633\n",
      "         _self_fruit\t-0.632\n",
      "              simple\t-0.624\n",
      "             veggies\t-0.610\n",
      "       _self_healthy\t-0.610\n",
      "                  vs\t-0.590\n",
      "                diet\t-0.583\n",
      "              sounds\t-0.559\n",
      "                 i'm\t-0.556\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 negative coefficient words:\")\n",
    "for i in np.argsort(comt_BOW_lr.coef_[0])[::1][:20]:\n",
    "    print('%20s\\t%.3f' % (comt_mentionAll_vectorizer.get_feature_names()[i], comt_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 5: Apply pre-trained classifiers to predict for unseen tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 5.1: Apply support classifier to classify all brands' tweets into support and non-support classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: this code do prediction for each brand, it takes about 8~10 hours to run for each dataset\n",
      "processing 1 brand: oldelpaso\n",
      "processing 2 brand: roman_meal\n",
      "processing 3 brand: seagate\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "sup_lr.fit(com_Best_feature,sup_healthlabel_list)\n",
    "Cause.healthbrand_predict_label_0_1(sup_lr,healthbrand_nameid,eco_terms,GN_model,health_keywords,sup_bow_vectorizer,sup_cont_vectorizer,\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_score_test.txt\",\n",
    "                             \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_23_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 5.2: Apply commitment classifier to classify all brands' support tweets into high- and low- commitment classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "healthbrand_predict_label_2_3() takes 7 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-7c634d162870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m Cause.healthbrand_predict_label_2_3(comt_lr,healthbrand_nameid,eco_terms,comt_mention1_vectorizer,comt_mentionAllpron_vectorizer,comt_topn_vectorizer,\n\u001b[1;32m      3\u001b[0m                               \u001b[0;34m\"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_23_test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                               \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_2_3_test.txt\")         \n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: healthbrand_predict_label_2_3() takes 7 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "comt_lr.fit(comt_com_Best_feature2,comt_healthlabel_list)\n",
    "Cause.healthbrand_predict_label_2_3(comt_lr,healthbrand_nameid,eco_terms,comt_mention1_vectorizer,comt_mentionAllpron_vectorizer,comt_topn_vectorizer,\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_23_test.txt\",\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_2_3_test.txt\")         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 6: aggregate each entity's cause-commitment tweets and compare with action score to find inauthentic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get data for 140 entities\n"
     ]
    }
   ],
   "source": [
    "entity_pred_info, entity_pred2_tw, entity_pred3_tw = Cause.get_aggregate_info(\"/data/2/zwang/2017_S/Tweet_Health/142brand_tweet_predict_proba_01_2_3.txt\",\n",
    "                                                         sim_limit=0.3,prob_limit=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 entities remain after remove entities (number of tweets<0)\n"
     ]
    }
   ],
   "source": [
    "remain_entity_predicts, remove_entity = Cause.filt_entity(entity_pred_info, healthbrand_score_dict, ntw_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 6.1: Apply different aggregation methods to select entities that have high word-ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "entity_n3,entity_frac3,entity_prob3,words_topn_entities = Cause.aggregation(remain_entity_predicts,topn=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-health]** section 6.2: Sort high word-rating entities by their action-rating and select top-n (high word-rating but low action-rating) as inauthentic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inauthentic_entities = Cause.inauthentic(words_topn_entities,healthbrand_score_dict,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity\taction_score\tn_label3\tfrac_label3\tprob_label3\n",
      "\n",
      "coffee_mate\t1.400000\t9\t1.000000\t0.930667\n",
      "ampenergy\t1.400000\t3\t0.428571\t0.847667\n",
      "monsterenergysa\t1.400000\t1\t1.000000\t0.746000\n",
      "sprite\t1.500000\t3\t1.000000\t0.855667\n",
      "littledebbie\t1.500000\t5\t0.312500\t0.876200\n",
      "powerade\t1.500000\t2\t1.000000\t0.826500\n",
      "cocacola\t1.500000\t3\t0.750000\t0.800333\n",
      "gatorade\t1.500000\t18\t0.720000\t0.786944\n",
      "haagendazs_us\t1.800000\t6\t1.000000\t0.887000\n",
      "hellmanns\t1.900000\t2\t0.400000\t0.926500\n"
     ]
    }
   ],
   "source": [
    "print(\"entity\\taction_score\\tn_label3\\tfrac_label3\\tprob_label3\\n\")\n",
    "for entity in inauthentic_entities:\n",
    "    print(\"%s\\t%f\\t%d\\t%f\\t%f\" % (entity,float(healthbrand_score_dict[entity]),int(entity_n3[entity]),float(entity_frac3[entity]),\n",
    "                                  float(entity_prob3[entity])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Dataset 2: brands with eco cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 1: select cause-relevant tweets as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 1017 brands with TGS\n"
     ]
    }
   ],
   "source": [
    "ecobrand_score_dict, ecobrand_sector,ecobrand_nameid = Cause.get_brand_info(ECO_SCORE,\"twitter\",\"TGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector distribution of eco brands:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'Apparel': 108,\n",
       "         'Appliances': 23,\n",
       "         'Car': 43,\n",
       "         'Electronics': 61,\n",
       "         'Food': 410,\n",
       "         'Household Che': 29,\n",
       "         'Lighting Prod': 3,\n",
       "         'Paper Product': 25,\n",
       "         'Personal Care': 298,\n",
       "         'Pet Food': 17})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sector distribution of eco brands:\")\n",
    "eco_sector = Counter()\n",
    "eco_sector.update(ecobrand_sector.values())\n",
    "eco_sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 500000 lines\n",
      "read 1000000 lines\n",
      "read 1500000 lines\n",
      "read 2000000 lines\n",
      "read 2500000 lines\n",
      "Collected 2624800 tweets for 966 eco brands in total.\n"
     ]
    }
   ],
   "source": [
    "ecobrand_tweets_dict = Cause.read_brand_tweets(BRAND_PATH+\"tweets.pruned.json.gz\", list(ecobrand_score_dict.keys()),\n",
    "                                            cause=\"eco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100 entities\n",
      "processed 200 entities\n",
      "processed 300 entities\n",
      "processed 400 entities\n",
      "processed 500 entities\n",
      "processed 600 entities\n",
      "processed 700 entities\n",
      "processed 800 entities\n",
      "processed 900 entities\n",
      "2280489 non-duplicate tweets for 966 eco brands\n"
     ]
    }
   ],
   "source": [
    "ecobrand_twID_dict, ecobrand_twID_twtext = Cause.dedup_tweets(ecobrand_tweets_dict,cause=\"eco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ecobrand_twID_twtext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2801d1370dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mecobrand_twID_twScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCause\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_tweet_by_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecobrand_twID_twtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGN_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meco_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ecobrand_twID_twtext' is not defined"
     ]
    }
   ],
   "source": [
    "ecobrand_twID_twScore = Cause.score_tweet_by_relevance(ecobrand_twID_twtext,GN_model,eco_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "healthbrand_twIDScore = Cause.sort_tweet_by_score(healthbrand_twID_twScore,healthbrand_twID_dict)\n",
    "#Cause.select_topn_tweets(filename,ecobrand_twIDScore,ecobrand_twID_twtext,topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 2 & 3: feature enginerring & train and evaluate support classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Labeled data for support classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 308 positive instances and 658 negative instances for support classification\n"
     ]
    }
   ],
   "source": [
    "sup_ecobrand_list, sup_ecotweet_list, sup_ecolabel_list = Cause.data_for_sup_clf(brand_eco_path, entity='eco-brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_eco_neg_terms, sup_eco_pos_terms = Cause.get_freq_terms(sup_ecotweet_list,sup_ecolabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (non-support):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 403),\n",
       " ('natural', 73),\n",
       " ('rt', 66),\n",
       " ('_NUMBER_', 45),\n",
       " ('amp', 39),\n",
       " ('us', 34),\n",
       " ('new', 32),\n",
       " ('habitat', 32),\n",
       " (\"it's\", 31),\n",
       " ('water', 28),\n",
       " ('food', 26),\n",
       " ('beautiful', 26),\n",
       " ('skin', 25),\n",
       " ('know', 25),\n",
       " ('beauty', 25),\n",
       " ('life', 25),\n",
       " ('great', 23),\n",
       " ('like', 23),\n",
       " ('forest', 23),\n",
       " ('nature', 22)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (non-support):\")\n",
    "sup_eco_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in positive class (support):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 218),\n",
       " ('environment', 48),\n",
       " ('_NUMBER_', 43),\n",
       " ('sustainable', 41),\n",
       " ('rt', 34),\n",
       " ('help', 30),\n",
       " ('amp', 29),\n",
       " ('learn', 26),\n",
       " ('planet', 25),\n",
       " ('environmental', 22),\n",
       " ('sustainability', 20),\n",
       " ('water', 20),\n",
       " ('earth', 19),\n",
       " ('climate', 18),\n",
       " ('support', 18),\n",
       " ('day', 17),\n",
       " ('protect', 16),\n",
       " (\"we're\", 16),\n",
       " ('energy', 15),\n",
       " ('committed', 15)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in positive class (support):\")\n",
    "sup_eco_pos_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 3.1: Evaluating linguistic features, word embedding features and combination of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_lr = LogisticRegression(solver = 'lbfgs',multi_class ='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bag-of-words\n",
    "sup_BOW_tweet = sup_ecotweet_list\n",
    "sup_bow_vectorizer,sup_BOW_tw_matrix = Cause.construct_feature_matrix(sup_BOW_tweet)\n",
    "\n",
    "#bag-of-words + polarity\n",
    "sup_BOW_addpola = Cause.mark_polarity(sup_ecotweet_list,to_wd=0)\n",
    "sup_bowneg_vectorizer,sup_BOW_neg_matrix = Cause.construct_feature_matrix(sup_BOW_addpola)\n",
    "\n",
    "#bag-of-words + pronoun\n",
    "sup_BOW_addPron_tweet = Cause.mark_pronouns(sup_ecotweet_list, binary = False)\n",
    "sup_pron_vectorizer,sup_BOW_pron_matrix = Cause.construct_feature_matrix(sup_BOW_addPron_tweet)\n",
    "\n",
    "#bag-of-words + keywords' context\n",
    "sup_BOW_addCont_tweet = Cause.mark_context(sup_ecotweet_list,eco_terms)\n",
    "sup_cont_vectorizer,sup_BOW_cont_matrix = Cause.construct_feature_matrix(sup_BOW_addCont_tweet)\n",
    "\n",
    "#bag-of-words + remove cause keywords\n",
    "sup_BOW_rmTopic_tweet = Cause.remove_keywords(sup_ecotweet_list, eco_terms)\n",
    "sup_rmTopic_vectorizer,sup_BOW_rmTopic_matrix = Cause.construct_feature_matrix(sup_BOW_rmTopic_tweet)\n",
    "\n",
    "#bag-of-words + self_mention\n",
    "sup_BOW_self_tweet_once = Cause.selfmention(sup_ecobrand_list,ecobrand_nameid,sup_ecotweet_list, count=\"once\")\n",
    "sup_mention1_vectorizer,sup_BOW_mentionOnce_matrix = Cause.construct_feature_matrix(sup_BOW_self_tweet_once)\n",
    "\n",
    "#bag-of-words + self_mention to all words\n",
    "sup_BOW_self_tweet_all = Cause.selfmention(sup_ecobrand_list,ecobrand_nameid,sup_ecotweet_list, count=\"all\")\n",
    "sup_mentionAll_vectorizer,sup_BOW_mentionAll_matrix = Cause.construct_feature_matrix(sup_BOW_self_tweet_all)\n",
    "\n",
    "#bag-of-words + self_mention + pronoun\n",
    "sup_BOW_self_pron_tweet = Cause.selfmention(sup_ecobrand_list,ecobrand_nameid,sup_BOW_addPron_tweet, count=\"once\")\n",
    "sup_mention1pron_vectorizer,sup_BOW_mentionOncePron_matrix = Cause.construct_feature_matrix(sup_BOW_self_pron_tweet)\n",
    "\n",
    "#bag-of-words + self_mention to all words + pronoun\n",
    "sup_BOW_selfall_pron_tweet = Cause.selfmention(sup_ecobrand_list,ecobrand_nameid,sup_BOW_addPron_tweet, count=\"all\")\n",
    "sup_mentionAllpron_vectorizer,sup_BOW_mentionAllPron_matrix = Cause.construct_feature_matrix(sup_BOW_selfall_pron_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_Lingu_feature_names = [\"sup_BOW_tw_matrix\",\"sup_BOW_neg_matrix\",\"sup_BOW_pron_matrix\",\"sup_BOW_cont_matrix\", \n",
    "                           \"sup_BOW_rmTopic_matrix\",\"sup_BOW_mentionOnce_matrix\",\"sup_BOW_mentionAll_matrix\", \n",
    "                           \"sup_BOW_mentionOncePron_matrix\", \"sup_BOW_mentionAllPron_matrix\"]\n",
    "\n",
    "sup_Lingu_features = [sup_BOW_tw_matrix, sup_BOW_neg_matrix, sup_BOW_pron_matrix, sup_BOW_cont_matrix, sup_BOW_rmTopic_matrix,\n",
    "        sup_BOW_mentionOnce_matrix, sup_BOW_mentionAll_matrix, sup_BOW_mentionOncePron_matrix, sup_BOW_mentionAllPron_matrix]\n",
    "sup_Lingu_feature_dict = {}\n",
    "for i in range(len(sup_Lingu_feature_names)):\n",
    "    sup_Lingu_feature_dict[sup_Lingu_feature_names[i]] = sup_Lingu_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup_BOW_neg_matrix\t0.716\n",
      "sup_BOW_cont_matrix\t0.716\n",
      "sup_BOW_tw_matrix\t0.713\n",
      "sup_BOW_pron_matrix\t0.711\n",
      "sup_BOW_mentionOnce_matrix\t0.709\n",
      "sup_BOW_mentionOncePron_matrix\t0.709\n",
      "sup_BOW_mentionAllPron_matrix\t0.675\n",
      "sup_BOW_mentionAll_matrix\t0.670\n",
      "sup_BOW_rmTopic_matrix\t0.651\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_bow_feature(sup_Lingu_feature_dict,sup_ecolabel_list,sup_lr,mycv,score_func='f1')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_W2V_tw_score = Cause.calculate_tw_w2v_score(sup_ecotweet_list,GN_model,eco_keywords)\n",
    "sup_W2V_tw_vector = Cause.construct_tw_vector(sup_ecotweet_list,GN_model)\n",
    "\n",
    "sup_tweet_rankedwd_list = Cause.rank_match_words(sup_ecotweet_list, GN_model, eco_keywords)\n",
    "sup_W2V_topnwd, sup_W2V_topnwd_scores = Cause.get_topn_words(sup_tweet_rankedwd_list,n=3)\n",
    "sup_topn_vectorizer,sup_W2V_topnwd_matrix = Cause.construct_feature_matrix(sup_W2V_topnwd)\n",
    "\n",
    "sup_W2V_topnwd_vectors = Cause.get_topn_vectors(sup_tweet_rankedwd_list,GN_model,n=3)\n",
    "\n",
    "sup_W2V_topicwd_list,sup_tweet_topicwd_tp_list = Cause.get_topic_words(sup_ecotweet_list,GN_model,eco_keywords,threshold = 0.30)\n",
    "sup_topic_vectorizer,sup_Topicwd_matrix = Cause.construct_feature_matrix(sup_W2V_topicwd_list)\n",
    "\n",
    "sup_W2V_topicwd_ct,sup_W2V_topicwd_score,sup_W2V_topicwd_leftcontri,sup_W2V_topicwd_rightcontri = Cause.sep_topic_features(sup_tweet_topicwd_tp_list)\n",
    "\n",
    "sup_W2V_topicwd_sum = Cause.get_topicwd_score_sum(sup_W2V_topicwd_score)\n",
    "\n",
    "sup_W2V_contri_score = Cause.get_contri_sum(sup_tweet_topicwd_tp_list)\n",
    "\n",
    "sup_W2V_topicwd_vectors = Cause.get_topicwd_vec(GN_model,sup_tweet_topicwd_tp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_W2V_feature_names = [\"sup_W2V_tw_score\", \"sup_W2V_tw_vector\", \"sup_W2V_topnwd_matrix\", \"sup_W2V_topnwd_scores\", \n",
    "                     \"sup_W2V_topnwd_vectors\", \"sup_Topicwd_matrix\", \"sup_W2V_topicwd_ct\", \"sup_W2V_topicwd_sum\", \n",
    "                     \"sup_W2V_contri_score\", \"sup_W2V_topicwd_vectors\"]\n",
    "\n",
    "sup_W2V_features = [sup_W2V_tw_score, sup_W2V_tw_vector, sup_W2V_topnwd_matrix, sup_W2V_topnwd_scores, sup_W2V_topnwd_vectors, \n",
    "                sup_Topicwd_matrix, sup_W2V_topicwd_ct, sup_W2V_topicwd_sum, sup_W2V_contri_score, sup_W2V_topicwd_vectors]\n",
    "sup_W2V_feature_dict = {}\n",
    "for i in range(len(sup_W2V_feature_names)):\n",
    "    sup_W2V_feature_dict[sup_W2V_feature_names[i]] = sup_W2V_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup_W2V_topnwd_vectors\t0.775\n",
      "sup_W2V_tw_vector\t0.742\n",
      "sup_W2V_topnwd_matrix\t0.684\n",
      "sup_Topicwd_matrix\t0.658\n",
      "sup_W2V_topicwd_vectors\t0.647\n",
      "sup_W2V_topicwd_ct\t0.433\n",
      "sup_W2V_topicwd_sum\t0.428\n",
      "sup_W2V_contri_score\t0.406\n",
      "sup_W2V_topnwd_scores\t0.393\n",
      "sup_W2V_tw_score\t0.254\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_w2v_feature(sup_W2V_feature_dict,sup_ecolabel_list,sup_lr,mycv,score_func='f1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_COM_feature_names = [\"sup_BOW_tw_matrix\",\"sup_BOW_neg_matrix\",\"sup_BOW_mentionOnce_matrix\",\"sup_BOW_cont_matrix\",\n",
    "                     \"sup_W2V_tw_vector\", \"sup_W2V_topnwd_matrix\", \"sup_W2V_topnwd_vectors\", \"sup_W2V_topicwd_vectors\"]\n",
    "\n",
    "sup_COM_features = [sup_BOW_tw_matrix,sup_BOW_neg_matrix,sup_BOW_mentionOnce_matrix,sup_BOW_cont_matrix,\n",
    "                sup_W2V_tw_vector, sup_W2V_topnwd_matrix, sup_W2V_topnwd_vectors, sup_W2V_topicwd_vectors]\n",
    "sup_COM_feature_dict = {}\n",
    "for i in range(len(sup_COM_feature_names)):\n",
    "    sup_COM_feature_dict[sup_COM_feature_names[i]] = sup_COM_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine 1 features.\n",
      "Combine 2 features.\n",
      "Combine 3 features.\n",
      "Combine 4 features.\n",
      "Combine 5 features.\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.822\n",
      "sup_BOW_tw_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.818\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.818\n",
      "sup_BOW_neg_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.817\n",
      "sup_BOW_mentionOnce_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.817\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors\t0.815\n",
      "sup_BOW_mentionOnce_matrix + sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.812\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.811\n",
      "sup_BOW_mentionOnce_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.811\n",
      "sup_BOW_mentionOnce_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors\t0.810\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_comb_feature(sup_COM_feature_names,sup_COM_feature_dict,sup_ecolabel_list,sup_lr,mycv,score_func='f1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.865\n",
      "recall:0.783\n",
      "f1:0.821\n"
     ]
    }
   ],
   "source": [
    "sup_com_Best_feature_names=[\"sup_BOW_cont_matrix\",\"sup_W2V_tw_vector\", \"sup_W2V_topnwd_vectors\"]\n",
    "sup_com_Best_feature_list = [sup_BOW_cont_matrix,sup_W2V_tw_vector, sup_W2V_topnwd_vectors]\n",
    "sup_com_Best_feature_dict = {}\n",
    "for i in range(len(sup_com_Best_feature_names)):\n",
    "    sup_com_Best_feature_dict[sup_com_Best_feature_names[i]] = sup_com_Best_feature_names[i]\n",
    "   \n",
    "sup_com_Best_feature = sup_com_Best_feature_list[0]\n",
    "for feature in sup_com_Best_feature_list[1:]:\n",
    "    sup_com_Best_feature = np.hstack((sup_com_Best_feature,feature))\n",
    "\n",
    "print(\"precision:%.3f\" % np.mean(cross_val_score(sup_lr, sup_com_Best_feature, sup_ecolabel_list,cv=mycv,scoring='precision')))\n",
    "print(\"recall:%.3f\" % np.mean(cross_val_score(sup_lr, sup_com_Best_feature, sup_ecolabel_list,cv=mycv,scoring='recall')))\n",
    "print(\"f1:%.3f\" % np.mean(cross_val_score(sup_lr, sup_com_Best_feature, sup_ecolabel_list,cv=mycv,scoring='f1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 2 & 4: feature enginerring & train and evaluate commitment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Labeled data for commitment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 148 positive instances and 160 negative instances for commitment classification\n"
     ]
    }
   ],
   "source": [
    "comt_ecobrand_list, comt_ecotweet_list, comt_ecolabel_list = Cause.data_for_commit_clf(brand_eco_path,entity='eco-brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_eco_neg_terms, comt_eco_pos_terms = Cause.get_freq_terms(comt_ecotweet_list,comt_ecolabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (low-commitment):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 118),\n",
       " ('environment', 23),\n",
       " ('rt', 22),\n",
       " ('planet', 20),\n",
       " ('_NUMBER_', 16),\n",
       " ('sustainable', 15),\n",
       " ('help', 14),\n",
       " ('day', 14),\n",
       " ('climate', 13),\n",
       " ('earth', 13),\n",
       " ('carbon', 12),\n",
       " ('amp', 11),\n",
       " ('great', 10),\n",
       " ('water', 10),\n",
       " ('future', 10),\n",
       " ('learn', 10),\n",
       " ('_HASHTAG_earthday', 10),\n",
       " ('plants', 9),\n",
       " ('know', 9),\n",
       " (\"it's\", 9)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (low-commitment):\")\n",
    "comt_eco_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (high-commitment):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 118),\n",
       " ('environment', 23),\n",
       " ('rt', 22),\n",
       " ('planet', 20),\n",
       " ('_NUMBER_', 16),\n",
       " ('sustainable', 15),\n",
       " ('help', 14),\n",
       " ('day', 14),\n",
       " ('climate', 13),\n",
       " ('earth', 13),\n",
       " ('carbon', 12),\n",
       " ('amp', 11),\n",
       " ('great', 10),\n",
       " ('water', 10),\n",
       " ('future', 10),\n",
       " ('learn', 10),\n",
       " ('_HASHTAG_earthday', 10),\n",
       " ('plants', 9),\n",
       " ('know', 9),\n",
       " (\"it's\", 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (high-commitment):\")\n",
    "comt_eco_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 4.1: Evaluating linguistic features, word embedding features and combination of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_lr = LogisticRegression(solver = 'lbfgs',multi_class ='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bag-of-words\n",
    "comt_BOW_tweet = comt_ecotweet_list\n",
    "comt_bow_vectorizer,comt_BOW_tw_matrix = Cause.construct_feature_matrix(comt_BOW_tweet)\n",
    "\n",
    "#bag-of-words + polarity\n",
    "comt_BOW_addpola = Cause.mark_polarity(comt_ecotweet_list,to_wd=0)\n",
    "comt_bowneg_vectorizer,comt_BOW_neg_matrix = Cause.construct_feature_matrix(comt_BOW_addpola)\n",
    "\n",
    "#bag-of-words + pronoun\n",
    "comt_BOW_addPron_tweet = Cause.mark_pronouns(comt_ecotweet_list, binary = False)\n",
    "comt_pron_vectorizer,comt_BOW_pron_matrix = Cause.construct_feature_matrix(comt_BOW_addPron_tweet)\n",
    "\n",
    "#bag-of-words + keywords' context\n",
    "comt_BOW_addCont_tweet = Cause.mark_context(comt_ecotweet_list,eco_terms)\n",
    "comt_cont_vectorizer,comt_BOW_cont_matrix = Cause.construct_feature_matrix(comt_BOW_addCont_tweet)\n",
    "\n",
    "#bag-of-words + remove cause keywords\n",
    "comt_BOW_rmTopic_tweet = Cause.remove_keywords(comt_ecotweet_list, eco_terms)\n",
    "comt_rmTopic_vectorizer,comt_BOW_rmTopic_matrix = Cause.construct_feature_matrix(comt_BOW_rmTopic_tweet)\n",
    "\n",
    "#bag-of-words + self_mention\n",
    "comt_BOW_self_tweet_once = Cause.selfmention(comt_ecobrand_list,ecobrand_nameid,comt_ecotweet_list, count=\"once\")\n",
    "comt_mention1_vectorizer,comt_BOW_mentionOnce_matrix = Cause.construct_feature_matrix(comt_BOW_self_tweet_once)\n",
    "\n",
    "#bag-of-words + self_mention to all words\n",
    "comt_BOW_self_tweet_all = Cause.selfmention(comt_ecobrand_list,ecobrand_nameid,comt_ecotweet_list, count=\"all\")\n",
    "comt_mentionAll_vectorizer,comt_BOW_mentionAll_matrix = Cause.construct_feature_matrix(comt_BOW_self_tweet_all)\n",
    "\n",
    "#bag-of-words + self_mention + pronoun\n",
    "comt_BOW_self_pron_tweet = Cause.selfmention(comt_ecobrand_list,ecobrand_nameid,comt_BOW_addPron_tweet, count=\"once\")\n",
    "comt_mention1pron_vectorizer,comt_BOW_mentionOncePron_matrix = Cause.construct_feature_matrix(comt_BOW_self_pron_tweet)\n",
    "\n",
    "#bag-of-words + self_mention to all words + pronoun\n",
    "comt_BOW_selfall_pron_tweet = Cause.selfmention(comt_ecobrand_list,ecobrand_nameid,comt_BOW_addPron_tweet, count=\"all\")\n",
    "comt_mentionAllpron_vectorizer,comt_BOW_mentionAllPron_matrix = Cause.construct_feature_matrix(comt_BOW_selfall_pron_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_Lingu_feature_names = [\"comt_BOW_tw_matrix\",\"comt_BOW_neg_matrix\",\"comt_BOW_pron_matrix\",\"comt_BOW_cont_matrix\", \n",
    "                           \"comt_BOW_rmTopic_matrix\",\"comt_BOW_mentionOnce_matrix\",\"comt_BOW_mentionAll_matrix\", \n",
    "                           \"comt_BOW_mentionOncePron_matrix\", \"comt_BOW_mentionAllPron_matrix\"]\n",
    "\n",
    "comt_Lingu_features = [comt_BOW_tw_matrix, comt_BOW_neg_matrix, comt_BOW_pron_matrix, comt_BOW_cont_matrix, comt_BOW_rmTopic_matrix,\n",
    "        comt_BOW_mentionOnce_matrix, comt_BOW_mentionAll_matrix, comt_BOW_mentionOncePron_matrix, comt_BOW_mentionAllPron_matrix]\n",
    "comt_Lingu_feature_dict = {}\n",
    "for i in range(len(comt_Lingu_feature_names)):\n",
    "    comt_Lingu_feature_dict[comt_Lingu_feature_names[i]] = comt_Lingu_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comt_BOW_mentionAllPron_matrix\t0.706\n",
      "comt_BOW_mentionOncePron_matrix\t0.698\n",
      "comt_BOW_mentionOnce_matrix\t0.670\n",
      "comt_BOW_rmTopic_matrix\t0.656\n",
      "comt_BOW_mentionAll_matrix\t0.654\n",
      "comt_BOW_pron_matrix\t0.646\n",
      "comt_BOW_cont_matrix\t0.639\n",
      "comt_BOW_neg_matrix\t0.612\n",
      "comt_BOW_tw_matrix\t0.612\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_bow_feature(comt_Lingu_feature_dict,comt_ecolabel_list,sup_lr,mycv,score_func='f1')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_W2V_tw_score = Cause.calculate_tw_w2v_score(comt_ecotweet_list,GN_model,eco_keywords)\n",
    "comt_W2V_tw_vector = Cause.construct_tw_vector(comt_ecotweet_list,GN_model)\n",
    "\n",
    "comt_tweet_rankedwd_list = Cause.rank_match_words(comt_ecotweet_list, GN_model, eco_keywords)\n",
    "comt_W2V_topnwd, comt_W2V_topnwd_scores = Cause.get_topn_words(comt_tweet_rankedwd_list,n=3)\n",
    "comt_topn_vectorizer,comt_W2V_topnwd_matrix = Cause.construct_feature_matrix(comt_W2V_topnwd)\n",
    "\n",
    "comt_W2V_topnwd_vectors = Cause.get_topn_vectors(comt_tweet_rankedwd_list,GN_model,n=3)\n",
    "\n",
    "comt_W2V_topicwd_list,comt_tweet_topicwd_tp_list = Cause.get_topic_words(comt_ecotweet_list,GN_model,eco_keywords,threshold = 0.30)\n",
    "comt_topic_vectorizer,comt_Topicwd_matrix = Cause.construct_feature_matrix(comt_W2V_topicwd_list)\n",
    "\n",
    "comt_W2V_topicwd_ct,comt_W2V_topicwd_score,comt_W2V_topicwd_leftcontri,comt_W2V_topicwd_rightcontri = Cause.sep_topic_features(comt_tweet_topicwd_tp_list)\n",
    "\n",
    "comt_W2V_topicwd_sum = Cause.get_topicwd_score_sum(comt_W2V_topicwd_score)\n",
    "\n",
    "comt_W2V_contri_score = Cause.get_contri_sum(comt_tweet_topicwd_tp_list)\n",
    "\n",
    "comt_W2V_topicwd_vectors = Cause.get_topicwd_vec(GN_model,comt_tweet_topicwd_tp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_W2V_feature_names = [\"comt_W2V_tw_vector\", \"comt_W2V_topnwd_matrix\",  \n",
    "                     \"comt_W2V_topnwd_vectors\", \"comt_Topicwd_matrix\", \"comt_W2V_topicwd_ct\", \"comt_W2V_topicwd_sum\", \n",
    "                     \"comt_W2V_contri_score\", \"comt_W2V_topicwd_vectors\"]\n",
    "\n",
    "comt_W2V_features = [comt_W2V_tw_vector, comt_W2V_topnwd_matrix, comt_W2V_topnwd_vectors, \n",
    "                comt_Topicwd_matrix, comt_W2V_topicwd_ct, comt_W2V_topicwd_sum, comt_W2V_contri_score, comt_W2V_topicwd_vectors]\n",
    "comt_W2V_feature_dict = {}\n",
    "for i in range(len(comt_W2V_feature_names)):\n",
    "    comt_W2V_feature_dict[comt_W2V_feature_names[i]] = comt_W2V_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comt_W2V_tw_vector\t0.653\n",
      "comt_W2V_topnwd_vectors\t0.587\n",
      "comt_W2V_topnwd_matrix\t0.581\n",
      "comt_Topicwd_matrix\t0.556\n",
      "comt_W2V_topicwd_vectors\t0.543\n",
      "comt_W2V_contri_score\t0.530\n",
      "comt_W2V_topicwd_ct\t0.413\n",
      "comt_W2V_topicwd_sum\t0.400\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_w2v_feature(comt_W2V_feature_dict,comt_ecolabel_list,comt_lr,mycv,score_func='f1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_COM_feature_names = [\"comt_BOW_tw_matrix\",\"comt_BOW_mentionOnce_matrix\",\"comt_BOW_mentionAllPron_matrix\",\"comt_BOW_rmTopic_matrix\",\n",
    "                     \"comt_W2V_tw_vector\", \"comt_W2V_topnwd_matrix\", \"comt_W2V_topnwd_vectors\", \"comt_W2V_topicwd_vectors\"]\n",
    "\n",
    "comt_COM_features = [comt_BOW_tw_matrix,comt_BOW_mentionOnce_matrix,comt_BOW_mentionAllPron_matrix,comt_BOW_rmTopic_matrix,\n",
    "                comt_W2V_tw_vector, comt_W2V_topnwd_matrix, comt_W2V_topnwd_vectors, comt_W2V_topicwd_vectors]\n",
    "comt_COM_feature_dict = {}\n",
    "for i in range(len(comt_COM_feature_names)):\n",
    "    comt_COM_feature_dict[comt_COM_feature_names[i]] = comt_COM_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine 1 features.\n",
      "Combine 2 features.\n",
      "Combine 3 features.\n",
      "Combine 4 features.\n",
      "Combine 5 features.\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix + comt_W2V_tw_vector\t0.721\n",
      "comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix\t0.718\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix\t0.713\n",
      "comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix\t0.712\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix\t0.709\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector\t0.709\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix\t0.709\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix\t0.708\n",
      "comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix + comt_W2V_topnwd_matrix\t0.708\n",
      "comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix\t0.706\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_comb_feature(comt_COM_feature_names,comt_COM_feature_dict,comt_ecolabel_list,comt_lr,mycv,score_func='f1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.773\n",
      "recall:0.677\n",
      "f1:0.721\n"
     ]
    }
   ],
   "source": [
    "comt_com_Best_feature_names=[\"comt_BOW_tw_matrix\",\"comt_BOW_mentionOnce_matrix\",\"comt_BOW_mentionAllPron_matrix\",\"comt_BOW_rmTopic_matrix\", \"comt_W2V_tw_vector\"]\n",
    "comt_com_Best_feature_list = [comt_BOW_tw_matrix,comt_BOW_mentionOnce_matrix,comt_BOW_mentionAllPron_matrix, comt_BOW_rmTopic_matrix,comt_W2V_tw_vector]\n",
    "comt_com_Best_feature_dict = {}\n",
    "for i in range(len(comt_com_Best_feature_names)):\n",
    "    comt_com_Best_feature_dict[comt_com_Best_feature_names[i]] = comt_com_Best_feature_names[i]\n",
    "   \n",
    "comt_com_Best_feature = comt_com_Best_feature_list[0]\n",
    "for feature in comt_com_Best_feature_list[1:]:\n",
    "    comt_com_Best_feature = np.hstack((comt_com_Best_feature,feature))\n",
    "\n",
    "print(\"precision:%.3f\" % np.mean(cross_val_score(sup_lr, comt_com_Best_feature, comt_ecolabel_list,cv=mycv,scoring='precision')))\n",
    "print(\"recall:%.3f\" % np.mean(cross_val_score(sup_lr, comt_com_Best_feature, comt_ecolabel_list,cv=mycv,scoring='recall')))\n",
    "print(\"f1:%.3f\" % np.mean(cross_val_score(sup_lr, comt_com_Best_feature, comt_ecolabel_list,cv=mycv,scoring='f1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 4.2: Evaluate different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(multi_class ='ovr',penalty='l2',class_weight=\"balanced\")\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "nn = MLPClassifier(solver='lbfgs',alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709\tLogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.602\tGaussianNB(priors=None)\n",
      "0.545\tRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "0.708\tMLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_classifier(com_Best_feature,comt_ecolabel_list,mycv,score_func='f1',classifier_list = [lr,gnb,rf,nn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 4.3: Analyze terms that have high coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comt_BOW_lr = LogisticRegression(penalty=\"l2\",class_weight=\"balanced\")\n",
    "comt_BOW_lr.fit(comt_BOW_mentionAllPron_matrix, comt_ecolabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 positive coefficient words:\n",
      "       first__person\t1.166\n",
      "               we're\t1.122\n",
      "           committed\t0.971\n",
      "             protect\t0.849\n",
      "                work\t0.719\n",
      "             restore\t0.717\n",
      "           _self_our\t0.698\n",
      "            _NUMBER_\t0.683\n",
      "             greener\t0.676\n",
      "           encourage\t0.636\n",
      "                palm\t0.628\n",
      "      sustainability\t0.627\n",
      " _self_first__person\t0.624\n",
      "                 oil\t0.560\n",
      "            _self_rt\t0.557\n",
      "              member\t0.542\n",
      "       environmental\t0.541\n",
      "               apple\t0.528\n",
      "                 new\t0.526\n",
      "                harm\t0.510\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 positive coefficient words:\")\n",
    "for i in np.argsort(comt_BOW_lr.coef_[0])[::-1][:20]:\n",
    "    print('%20s\\t%.3f' % (comt_mentionAllpron_vectorizer.get_feature_names()[i], comt_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 negative coefficient words:\n",
      "              planet\t-0.978\n",
      "                  rt\t-0.906\n",
      "              plants\t-0.731\n",
      "       third__person\t-0.729\n",
      "              _self_\t-0.712\n",
      "              oceans\t-0.695\n",
      "                 day\t-0.692\n",
      "      second__person\t-0.690\n",
      "               great\t-0.687\n",
      "                life\t-0.649\n",
      "               today\t-0.623\n",
      "              carbon\t-0.573\n",
      "                   3\t-0.572\n",
      "         responsible\t-0.568\n",
      "             climate\t-0.534\n",
      "          pesticides\t-0.532\n",
      "                home\t-0.496\n",
      "            reducing\t-0.479\n",
      "               ideas\t-0.479\n",
      "             natural\t-0.466\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 negative coefficient words:\")\n",
    "for i in np.argsort(comt_BOW_lr.coef_[0])[::1][:20]:\n",
    "    print('%20s\\t%.3f' % (comt_mentionAllpron_vectorizer.get_feature_names()[i], comt_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 5: Apply pre-trained classifiers to predict for unseen tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 5.1: Apply support classifier to classify all brands' tweets into support and non-support classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_lr.fit(sup_com_Best_feature,sup_ecolabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: this code do prediction for each brand, it takes about 8~10 hours to run for each dataset\n",
      "processing 1 brand: oldelpaso\n",
      "processing 2 brand: roman_meal\n",
      "processing 3 brand: seagate\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "Cause.ecobrand_predict_label_0_1(sup_lr,ecobrand_nameid,eco_terms,GN_model,eco_keywords,sup_cont_vectorizer,\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_score_test.txt\",\n",
    "                             \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_23_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 5.2: Apply commitment classifier to classify all brands' support tweets into high- and low- commitment classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comt_lr.fit(comt_com_Best_feature,comt_ecolabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Cause.ecobrand_predict_label_2_3(comt_lr,GN_model,ecobrand_nameid,eco_terms,eco_keywords,comt_bow_vectorizer,comt_mention1_vectorizer,comt_mentionAllpron_vectorizer,comt_rmTopic_vectorizer,\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_23_test.txt\",\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_2_3_test.txt\")         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 6: aggregate each entity's cause-commitment tweets and compare with action score to find inauthentic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get data for 922 entities\n"
     ]
    }
   ],
   "source": [
    "entity_pred_info, entity_pred2_tw, entity_pred3_tw = Cause.get_aggregate_info(\"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_predict_proba_01_2_3.txt\",\n",
    "                                                         sim_limit=0.3,prob_limit=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 entities remain after remove entities (number of tweets<0)\n"
     ]
    }
   ],
   "source": [
    "remain_entity_predicts, remove_entity = Cause.filt_entity(entity_pred_info, healthbrand_score_dict, ntw_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 6.1: Apply different aggregation methods to select entities that have high word-ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "entity_n3,entity_frac3,entity_prob3,words_topn_entities = Cause.aggregation(remain_entity_predicts,topn=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[brands-eco]** section 6.2: Sort high word-rating entities by their action-rating and select top-n (high word-rating but low action-rating) as inauthentic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity\taction_score\tn_label3\tfrac_label3\tprob_label3\n",
      "\n",
      "lovemyphilly\t\t0\t0.000000\t0.000000\n",
      "pomega5\t\t1\t0.012048\t0.977000\n",
      "thefrownies\t\t0\t0.000000\t0.000000\n",
      "miraclewhip\t\t0\t0.000000\t0.000000\n",
      "mdmoms\t\t2\t0.045455\t0.918500\n",
      "eatwholly\t\t0\t0.000000\t0.000000\n",
      "gourmet\t3.2\t1\t0.016129\t0.717000\n",
      "manentailbeauty\t3.2\t0\t0.000000\t0.000000\n",
      "butterlondon\t3.2\t1\t0.166667\t0.911000\n",
      "bronnerbros\t3.2\t0\t0.000000\t0.000000\n"
     ]
    }
   ],
   "source": [
    "inauthentic_entities = Cause.inauthentic(words_topn_entities,ecobrand_score_dict,n=10)\n",
    "print(\"entity\\taction_score\\tn_label3\\tfrac_label3\\tprob_label3\\n\")\n",
    "for entity in inauthentic_entities:\n",
    "    print(\"%s\\t%s\\t%d\\t%f\\t%f\" % (entity,ecobrand_score_dict[entity],int(entity_n3[entity]),float(entity_frac3[entity]),\n",
    "                                  float(entity_prob3[entity])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Dataset 3: member of Congress (MOC for short) with eco cause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[MOC-eco]** section 1: select cause-relevant tweets as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read tweets for 100 congress members\n",
      "Read tweets for 200 congress members\n",
      "Read tweets for 300 congress members\n",
      "Read tweets for 400 congress members\n",
      "Reapeted congress member: AustinScottGA08\n",
      "Reapeted congress member: RepDannyDavis\n",
      "Reapeted congress member: RepMaloney\n",
      "Reapeted congress member: RepAlGreen\n",
      "Reapeted congress member: RepEBJ\n",
      "Read tweets for 500 congress members\n",
      "Reapeted congress member: LorettaSanchez\n",
      "Collected 1118962 tweets for 514 congress members in total.\n"
     ]
    }
   ],
   "source": [
    "MOC_nameid_dict, MOC_rating, MOC_party_dict, MOC_state_dict, MOC_tweets_dict = Cause.read_moc_tweets(CONGRESS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100 entities\n",
      "processed 200 entities\n",
      "processed 300 entities\n",
      "processed 400 entities\n",
      "processed 500 entities\n",
      "1096604 non-duplicate tweets for 514 eco brands\n"
     ]
    }
   ],
   "source": [
    "MOC_twID_dict, MOC_twID_twtext = Cause.dedup_tweets(MOC_tweets_dict,cause=\"eco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MOC_twID_twScore = Cause.score_tweet_by_relevance(MOC_twID_twtext,GN_model,eco_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MOC_twIDScore = Cause.sort_tweet_by_score(MOC_twID_twScore,MOC_twID_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Cause.select_topn_tweets(filename,MOC_twIDScore,MOC_twID_twtext,topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[MOC-eco]** section 2 & 3: feature enginerring & train and evaluate support classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Labeled data for support classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 379 positive instances and 133 negative instances for support classification\n"
     ]
    }
   ],
   "source": [
    "sup_moc_list, sup_moctweet_list, sup_moclabel_list = Cause.data_for_sup_clf(moc_eco_path, entity='eco-moc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_moc_neg_terms, sup_moc_pos_terms = Cause.get_freq_terms(sup_moctweet_list,sup_moclabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (non-support):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 87),\n",
       " ('rt', 18),\n",
       " ('amp', 16),\n",
       " ('economy', 15),\n",
       " ('jobs', 14),\n",
       " ('energy', 13),\n",
       " ('_NUMBER_', 12),\n",
       " ('water', 10),\n",
       " ('climate', 9),\n",
       " ('communities', 9),\n",
       " ('economic', 9),\n",
       " ('epa', 8),\n",
       " ('lives', 8),\n",
       " ('forest', 7),\n",
       " ('need', 7),\n",
       " ('industry', 7),\n",
       " ('world', 7),\n",
       " ('environment', 7),\n",
       " ('families', 7),\n",
       " ('nation', 7)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (non-support):\")\n",
    "sup_moc_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in positive class (support):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 255),\n",
       " ('amp', 92),\n",
       " ('protect', 57),\n",
       " ('climate', 51),\n",
       " ('wildlife', 49),\n",
       " ('species', 49),\n",
       " ('rt', 41),\n",
       " ('conservation', 37),\n",
       " ('change', 36),\n",
       " ('environment', 36),\n",
       " ('_NUMBER_', 33),\n",
       " ('water', 33),\n",
       " ('pollution', 31),\n",
       " ('health', 28),\n",
       " ('forests', 27),\n",
       " ('protecting', 26),\n",
       " ('endangered', 26),\n",
       " ('habitat', 25),\n",
       " ('future', 25),\n",
       " ('today', 24)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in positive class (support):\")\n",
    "sup_moc_pos_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[MOC-eco]** section 3.1: Evaluating linguistic features, word embedding features and combination of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_lr = LogisticRegression(solver = 'lbfgs',multi_class ='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bag-of-words\n",
    "sup_BOW_tweet = sup_moctweet_list\n",
    "sup_bow_vectorizer,sup_BOW_tw_matrix = Cause.construct_feature_matrix(sup_BOW_tweet)\n",
    "\n",
    "#bag-of-words + polarity\n",
    "sup_BOW_addpola = Cause.mark_polarity(sup_moctweet_list,to_wd=0)\n",
    "sup_bowneg_vectorizer,sup_BOW_neg_matrix = Cause.construct_feature_matrix(sup_BOW_addpola)\n",
    "\n",
    "#bag-of-words + pronoun\n",
    "sup_BOW_addPron_tweet = Cause.mark_pronouns(sup_moctweet_list, binary = False)\n",
    "sup_pron_vectorizer,sup_BOW_pron_matrix = Cause.construct_feature_matrix(sup_BOW_addPron_tweet)\n",
    "\n",
    "#bag-of-words + keywords' context\n",
    "sup_BOW_addCont_tweet = Cause.mark_context(sup_moctweet_list,eco_terms)\n",
    "sup_cont_vectorizer,sup_BOW_cont_matrix = Cause.construct_feature_matrix(sup_BOW_addCont_tweet)\n",
    "\n",
    "#bag-of-words + remove cause keywords\n",
    "sup_BOW_rmTopic_tweet = Cause.remove_keywords(sup_moctweet_list, eco_terms)\n",
    "sup_rmTopic_vectorizer,sup_BOW_rmTopic_matrix = Cause.construct_feature_matrix(sup_BOW_rmTopic_tweet)\n",
    "\n",
    "#bag-of-words + self_mention\n",
    "sup_BOW_self_tweet_once = Cause.selfmention(sup_moc_list,MOC_nameid_dict,sup_moctweet_list, count=\"once\")\n",
    "sup_mention1_vectorizer,sup_BOW_mentionOnce_matrix = Cause.construct_feature_matrix(sup_BOW_self_tweet_once)\n",
    "\n",
    "#bag-of-words + self_mention to all words\n",
    "sup_BOW_self_tweet_all = Cause.selfmention(sup_moc_list,MOC_nameid_dict,sup_moctweet_list, count=\"all\")\n",
    "sup_mentionAll_vectorizer,sup_BOW_mentionAll_matrix = Cause.construct_feature_matrix(sup_BOW_self_tweet_all)\n",
    "\n",
    "#bag-of-words + self_mention + pronoun\n",
    "sup_BOW_self_pron_tweet = Cause.selfmention(sup_moc_list,MOC_nameid_dict,sup_BOW_addPron_tweet, count=\"once\")\n",
    "sup_mention1pron_vectorizer,sup_BOW_mentionOncePron_matrix = Cause.construct_feature_matrix(sup_BOW_self_pron_tweet)\n",
    "\n",
    "#bag-of-words + self_mention to all words + pronoun\n",
    "sup_BOW_selfall_pron_tweet = Cause.selfmention(sup_moc_list,MOC_nameid_dict,sup_BOW_addPron_tweet, count=\"all\")\n",
    "sup_mentionAllpron_vectorizer,sup_BOW_mentionAllPron_matrix = Cause.construct_feature_matrix(sup_BOW_selfall_pron_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_Lingu_feature_names = [\"sup_BOW_tw_matrix\",\"sup_BOW_neg_matrix\",\"sup_BOW_pron_matrix\",\"sup_BOW_cont_matrix\", \n",
    "                           \"sup_BOW_rmTopic_matrix\",\"sup_BOW_mentionOnce_matrix\",\"sup_BOW_mentionAll_matrix\", \n",
    "                           \"sup_BOW_mentionOncePron_matrix\", \"sup_BOW_mentionAllPron_matrix\"]\n",
    "\n",
    "sup_Lingu_features = [sup_BOW_tw_matrix, sup_BOW_neg_matrix, sup_BOW_pron_matrix, sup_BOW_cont_matrix, sup_BOW_rmTopic_matrix,\n",
    "        sup_BOW_mentionOnce_matrix, sup_BOW_mentionAll_matrix, sup_BOW_mentionOncePron_matrix, sup_BOW_mentionAllPron_matrix]\n",
    "sup_Lingu_feature_dict = {}\n",
    "for i in range(len(sup_Lingu_feature_names)):\n",
    "    sup_Lingu_feature_dict[sup_Lingu_feature_names[i]] = sup_Lingu_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup_BOW_mentionAll_matrix\t0.873\n",
      "sup_BOW_mentionOnce_matrix\t0.873\n",
      "sup_BOW_tw_matrix\t0.873\n",
      "sup_BOW_cont_matrix\t0.872\n",
      "sup_BOW_neg_matrix\t0.871\n",
      "sup_BOW_mentionAllPron_matrix\t0.871\n",
      "sup_BOW_rmTopic_matrix\t0.869\n",
      "sup_BOW_mentionOncePron_matrix\t0.868\n",
      "sup_BOW_pron_matrix\t0.868\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_bow_feature(sup_Lingu_feature_dict,sup_moclabel_list,sup_lr,mycv,score_func='f1')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_W2V_tw_score = Cause.calculate_tw_w2v_score(sup_moctweet_list,GN_model,eco_keywords)\n",
    "sup_W2V_tw_vector = Cause.construct_tw_vector(sup_moctweet_list,GN_model)\n",
    "\n",
    "sup_tweet_rankedwd_list = Cause.rank_match_words(sup_moctweet_list, GN_model, eco_keywords)\n",
    "sup_W2V_topnwd, sup_W2V_topnwd_scores = Cause.get_topn_words(sup_tweet_rankedwd_list,n=3)\n",
    "sup_topn_vectorizer,sup_W2V_topnwd_matrix = Cause.construct_feature_matrix(sup_W2V_topnwd)\n",
    "\n",
    "sup_W2V_topnwd_vectors = Cause.get_topn_vectors(sup_tweet_rankedwd_list,GN_model,n=3)\n",
    "\n",
    "sup_W2V_topicwd_list,sup_tweet_topicwd_tp_list = Cause.get_topic_words(sup_moctweet_list,GN_model,eco_keywords,threshold = 0.30)\n",
    "sup_topic_vectorizer,sup_Topicwd_matrix = Cause.construct_feature_matrix(sup_W2V_topicwd_list)\n",
    "\n",
    "sup_W2V_topicwd_ct,sup_W2V_topicwd_score,sup_W2V_topicwd_leftcontri,sup_W2V_topicwd_rightcontri = Cause.sep_topic_features(sup_tweet_topicwd_tp_list)\n",
    "\n",
    "sup_W2V_topicwd_sum = Cause.get_topicwd_score_sum(sup_W2V_topicwd_score)\n",
    "\n",
    "sup_W2V_contri_score = Cause.get_contri_sum(sup_tweet_topicwd_tp_list)\n",
    "\n",
    "sup_W2V_topicwd_vectors = Cause.get_topicwd_vec(GN_model,sup_tweet_topicwd_tp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_W2V_feature_names = [\"sup_W2V_tw_score\", \"sup_W2V_tw_vector\", \"sup_W2V_topnwd_matrix\", \"sup_W2V_topnwd_scores\", \n",
    "                     \"sup_W2V_topnwd_vectors\", \"sup_Topicwd_matrix\", \"sup_W2V_topicwd_ct\", \"sup_W2V_topicwd_sum\", \n",
    "                     \"sup_W2V_contri_score\", \"sup_W2V_topicwd_vectors\"]\n",
    "\n",
    "sup_W2V_features = [sup_W2V_tw_score, sup_W2V_tw_vector, sup_W2V_topnwd_matrix, sup_W2V_topnwd_scores, sup_W2V_topnwd_vectors, \n",
    "                sup_Topicwd_matrix, sup_W2V_topicwd_ct, sup_W2V_topicwd_sum, sup_W2V_contri_score, sup_W2V_topicwd_vectors]\n",
    "sup_W2V_feature_dict = {}\n",
    "for i in range(len(sup_W2V_feature_names)):\n",
    "    sup_W2V_feature_dict[sup_W2V_feature_names[i]] = sup_W2V_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sup_W2V_tw_vector\t0.881\n",
      "sup_W2V_topicwd_vectors\t0.878\n",
      "sup_W2V_topnwd_vectors\t0.875\n",
      "sup_W2V_topnwd_scores\t0.871\n",
      "sup_W2V_topnwd_matrix\t0.864\n",
      "sup_Topicwd_matrix\t0.863\n",
      "sup_W2V_topicwd_sum\t0.862\n",
      "sup_W2V_contri_score\t0.857\n",
      "sup_W2V_tw_score\t0.851\n",
      "sup_W2V_topicwd_ct\t0.849\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_w2v_feature(sup_W2V_feature_dict,sup_moclabel_list,sup_lr,mycv,score_func='f1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sup_COM_feature_names = [\"sup_BOW_tw_matrix\",\"sup_BOW_neg_matrix\",\"sup_BOW_mentionOnce_matrix\",\"sup_BOW_cont_matrix\",\n",
    "                     \"sup_W2V_tw_vector\", \"sup_W2V_topnwd_matrix\", \"sup_W2V_topnwd_vectors\", \"sup_W2V_topicwd_vectors\"]\n",
    "\n",
    "sup_COM_features = [sup_BOW_tw_matrix,sup_BOW_neg_matrix,sup_BOW_mentionOnce_matrix,sup_BOW_cont_matrix,\n",
    "                sup_W2V_tw_vector, sup_W2V_topnwd_matrix, sup_W2V_topnwd_vectors, sup_W2V_topicwd_vectors]\n",
    "sup_COM_feature_dict = {}\n",
    "for i in range(len(sup_COM_feature_names)):\n",
    "    sup_COM_feature_dict[sup_COM_feature_names[i]] = sup_COM_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine 1 features.\n",
      "Combine 2 features.\n",
      "Combine 3 features.\n",
      "Combine 4 features.\n",
      "Combine 5 features.\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.915\n",
      "sup_BOW_cont_matrix + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.914\n",
      "sup_BOW_mentionOnce_matrix + sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.912\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.911\n",
      "sup_BOW_neg_matrix + sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.910\n",
      "sup_BOW_tw_matrix + sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.910\n",
      "sup_BOW_cont_matrix + sup_W2V_topnwd_vectors\t0.909\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_vectors\t0.908\n",
      "sup_BOW_mentionOnce_matrix + sup_BOW_cont_matrix + sup_W2V_topnwd_vectors + sup_W2V_topicwd_vectors\t0.907\n",
      "sup_BOW_cont_matrix + sup_W2V_tw_vector + sup_W2V_topnwd_matrix + sup_W2V_topnwd_vectors\t0.907\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_comb_feature(sup_COM_feature_names,sup_COM_feature_dict,sup_moclabel_list,sup_lr,mycv,score_func='f1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.879\n",
      "recall:0.952\n",
      "f1:0.913\n"
     ]
    }
   ],
   "source": [
    "sup_com_Best_feature_names=[\"sup_BOW_cont_matrix\",\"sup_W2V_tw_vector\", \"sup_W2V_topnwd_vectors\",\"sup_W2V_topicwd_vectors\"]\n",
    "sup_com_Best_feature_list = [sup_BOW_cont_matrix,sup_W2V_tw_vector, sup_W2V_topnwd_vectors,sup_W2V_topicwd_vectors]\n",
    "sup_com_Best_feature_dict = {}\n",
    "for i in range(len(sup_com_Best_feature_names)):\n",
    "    sup_com_Best_feature_dict[sup_com_Best_feature_names[i]] = sup_com_Best_feature_names[i]\n",
    "   \n",
    "sup_com_Best_feature = sup_com_Best_feature_list[0]\n",
    "for feature in sup_com_Best_feature_list[1:]:\n",
    "    sup_com_Best_feature = np.hstack((sup_com_Best_feature,feature))\n",
    "\n",
    "print(\"precision:%.3f\" % np.mean(cross_val_score(sup_lr, sup_com_Best_feature, sup_moclabel_list,cv=mycv,scoring='precision')))\n",
    "print(\"recall:%.3f\" % np.mean(cross_val_score(sup_lr, sup_com_Best_feature, sup_moclabel_list,cv=mycv,scoring='recall')))\n",
    "print(\"f1:%.3f\" % np.mean(cross_val_score(sup_lr, sup_com_Best_feature, sup_moclabel_list,cv=mycv,scoring='f1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[MOC-eco]** section 2 & 4: feature enginerring & train and evaluate commitment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> Labeled data for commitment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 140 positive instances and 239 negative instances for commitment classification\n"
     ]
    }
   ],
   "source": [
    "comt_moc_list, comt_moctweet_list, comt_moclabel_list = Cause.data_for_commit_clf(moc_eco_path,entity='eco-moc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_moc_neg_terms, comt_moc_pos_terms = Cause.get_freq_terms(comt_moctweet_list,comt_moclabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (low-commitment):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 154),\n",
       " ('amp', 58),\n",
       " ('species', 33),\n",
       " ('climate', 32),\n",
       " ('protect', 30),\n",
       " ('wildlife', 28),\n",
       " ('pollution', 25),\n",
       " ('_NUMBER_', 23),\n",
       " ('water', 23),\n",
       " ('environment', 22),\n",
       " ('change', 22),\n",
       " ('rt', 21),\n",
       " ('epa', 20),\n",
       " ('conservation', 19),\n",
       " ('forests', 18),\n",
       " ('endangered', 18),\n",
       " ('health', 18),\n",
       " ('carbon', 17),\n",
       " ('communities', 17),\n",
       " ('economy', 16)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (low-commitment):\")\n",
    "comt_moc_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common terms in negative class (high-commitment):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_URL_', 154),\n",
       " ('amp', 58),\n",
       " ('species', 33),\n",
       " ('climate', 32),\n",
       " ('protect', 30),\n",
       " ('wildlife', 28),\n",
       " ('pollution', 25),\n",
       " ('_NUMBER_', 23),\n",
       " ('water', 23),\n",
       " ('environment', 22),\n",
       " ('change', 22),\n",
       " ('rt', 21),\n",
       " ('epa', 20),\n",
       " ('conservation', 19),\n",
       " ('forests', 18),\n",
       " ('endangered', 18),\n",
       " ('health', 18),\n",
       " ('carbon', 17),\n",
       " ('communities', 17),\n",
       " ('economy', 16)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most common terms in negative class (high-commitment):\")\n",
    "comt_moc_neg_terms.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[moc-eco]** section 4.1: Evaluating linguistic features, word embedding features and combination of various features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_lr = LogisticRegression(solver = 'lbfgs',multi_class ='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#bag-of-words\n",
    "comt_BOW_tweet = comt_moctweet_list\n",
    "comt_bow_vectorizer,comt_BOW_tw_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_tweet)\n",
    "\n",
    "#bag-of-words + polarity\n",
    "comt_BOW_addpola = Cause.mark_polarity(comt_moctweet_list,to_wd=0)\n",
    "comt_bowneg_vectorizer,comt_BOW_neg_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_addpola)\n",
    "\n",
    "#bag-of-words + pronoun\n",
    "comt_BOW_addPron_tweet = Cause.mark_pronouns(comt_moctweet_list, binary = False)\n",
    "comt_pron_vectorizer,comt_BOW_pron_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_addPron_tweet)\n",
    "\n",
    "#bag-of-words + keywords' context\n",
    "comt_BOW_addCont_tweet = Cause.mark_context(comt_moctweet_list,eco_terms)\n",
    "comt_cont_vectorizer,comt_BOW_cont_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_addCont_tweet)\n",
    "\n",
    "#bag-of-words + remove cause keywords\n",
    "comt_BOW_rmTopic_tweet = Cause.remove_keywords(comt_moctweet_list, eco_terms)\n",
    "comt_rmTopic_vectorizer,comt_BOW_rmTopic_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_rmTopic_tweet)\n",
    "\n",
    "#bag-of-words + self_mention\n",
    "comt_BOW_self_tweet_once = Cause.selfmention(comt_moc_list,MOC_nameid_dict,comt_moctweet_list, count=\"once\")\n",
    "comt_mention1_vectorizer,comt_BOW_mentionOnce_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_self_tweet_once)\n",
    "\n",
    "#bag-of-words + self_mention to all words\n",
    "comt_BOW_self_tweet_all = Cause.selfmention(comt_moc_list,MOC_nameid_dict,comt_moctweet_list, count=\"all\")\n",
    "comt_mentionAll_vectorizer,comt_BOW_mentionAll_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_self_tweet_all)\n",
    "\n",
    "#bag-of-words + self_mention + pronoun\n",
    "comt_BOW_self_pron_tweet = Cause.selfmention(comt_moc_list,MOC_nameid_dict,comt_BOW_addPron_tweet, count=\"once\")\n",
    "comt_mention1pron_vectorizer,comt_BOW_mentionOncePron_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_self_pron_tweet)\n",
    "\n",
    "#bag-of-words + self_mention to all words + pronoun\n",
    "comt_BOW_selfall_pron_tweet = Cause.selfmention(comt_moc_list,MOC_nameid_dict,comt_BOW_addPron_tweet, count=\"all\")\n",
    "comt_mentionAllpron_vectorizer,comt_BOW_mentionAllPron_matrix = Cause.construct_feature_matrix_formoc(comt_BOW_selfall_pron_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_Lingu_feature_names = [\"comt_BOW_tw_matrix\",\"comt_BOW_neg_matrix\",\"comt_BOW_pron_matrix\",\"comt_BOW_cont_matrix\", \n",
    "                           \"comt_BOW_rmTopic_matrix\",\"comt_BOW_mentionOnce_matrix\",\"comt_BOW_mentionAll_matrix\", \n",
    "                           \"comt_BOW_mentionOncePron_matrix\", \"comt_BOW_mentionAllPron_matrix\"]\n",
    "\n",
    "comt_Lingu_features = [comt_BOW_tw_matrix, comt_BOW_neg_matrix, comt_BOW_pron_matrix, comt_BOW_cont_matrix, comt_BOW_rmTopic_matrix,\n",
    "        comt_BOW_mentionOnce_matrix, comt_BOW_mentionAll_matrix, comt_BOW_mentionOncePron_matrix, comt_BOW_mentionAllPron_matrix]\n",
    "comt_Lingu_feature_dict = {}\n",
    "for i in range(len(comt_Lingu_feature_names)):\n",
    "    comt_Lingu_feature_dict[comt_Lingu_feature_names[i]] = comt_Lingu_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comt_BOW_mentionAll_matrix\t0.651\n",
      "comt_BOW_mentionAllPron_matrix\t0.646\n",
      "comt_BOW_cont_matrix\t0.630\n",
      "comt_BOW_mentionOnce_matrix\t0.628\n",
      "comt_BOW_mentionOncePron_matrix\t0.628\n",
      "comt_BOW_rmTopic_matrix\t0.616\n",
      "comt_BOW_pron_matrix\t0.587\n",
      "comt_BOW_neg_matrix\t0.587\n",
      "comt_BOW_tw_matrix\t0.587\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_bow_feature(comt_Lingu_feature_dict,comt_moclabel_list,sup_lr,mycv,score_func='f1')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_W2V_tw_score = Cause.calculate_tw_w2v_score(comt_moctweet_list,GN_model,eco_keywords)\n",
    "comt_W2V_tw_vector = Cause.construct_tw_vector(comt_moctweet_list,GN_model)\n",
    "\n",
    "comt_tweet_rankedwd_list = Cause.rank_match_words(comt_moctweet_list, GN_model, eco_keywords)\n",
    "comt_W2V_topnwd, comt_W2V_topnwd_scores = Cause.get_topn_words(comt_tweet_rankedwd_list,n=3)\n",
    "comt_topn_vectorizer,comt_W2V_topnwd_matrix = Cause.construct_feature_matrix_formoc(comt_W2V_topnwd)\n",
    "\n",
    "comt_W2V_topnwd_vectors = Cause.get_topn_vectors(comt_tweet_rankedwd_list,GN_model,n=3)\n",
    "\n",
    "comt_W2V_topicwd_list,comt_tweet_topicwd_tp_list = Cause.get_topic_words(comt_moctweet_list,GN_model,eco_keywords,threshold = 0.30)\n",
    "comt_topic_vectorizer,comt_Topicwd_matrix = Cause.construct_feature_matrix_formoc(comt_W2V_topicwd_list)\n",
    "\n",
    "comt_W2V_topicwd_ct,comt_W2V_topicwd_score,comt_W2V_topicwd_leftcontri,comt_W2V_topicwd_rightcontri = Cause.sep_topic_features(comt_tweet_topicwd_tp_list)\n",
    "\n",
    "comt_W2V_topicwd_sum = Cause.get_topicwd_score_sum(comt_W2V_topicwd_score)\n",
    "\n",
    "comt_W2V_contri_score = Cause.get_contri_sum(comt_tweet_topicwd_tp_list)\n",
    "\n",
    "comt_W2V_topicwd_vectors = Cause.get_topicwd_vec(GN_model,comt_tweet_topicwd_tp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_W2V_feature_names = [\"comt_W2V_tw_vector\", \"comt_W2V_topnwd_matrix\",  \"comt_W2V_topnwd_vectors\", \"comt_Topicwd_matrix\", \n",
    "                      \"comt_W2V_topicwd_vectors\"]\n",
    "\n",
    "comt_W2V_features = [comt_W2V_tw_vector, comt_W2V_topnwd_matrix, comt_W2V_topnwd_vectors, \n",
    "                comt_Topicwd_matrix, comt_W2V_topicwd_vectors]\n",
    "comt_W2V_feature_dict = {}\n",
    "for i in range(len(comt_W2V_feature_names)):\n",
    "    comt_W2V_feature_dict[comt_W2V_feature_names[i]] = comt_W2V_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comt_W2V_tw_vector\t0.448\n",
      "comt_Topicwd_matrix\t0.292\n",
      "comt_W2V_topnwd_vectors\t0.284\n",
      "comt_W2V_topnwd_matrix\t0.274\n",
      "comt_W2V_topicwd_vectors\t0.264\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_w2v_feature(comt_W2V_feature_dict,comt_moclabel_list,comt_lr,mycv,score_func='f1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "comt_COM_feature_names = [\"comt_BOW_tw_matrix\",\"comt_BOW_mentionOnce_matrix\",\"comt_BOW_mentionAllPron_matrix\",\"comt_BOW_rmTopic_matrix\",\n",
    "                     \"comt_W2V_tw_vector\", \"comt_W2V_topnwd_matrix\", \"comt_W2V_topnwd_vectors\", \"comt_W2V_topicwd_vectors\"]\n",
    "\n",
    "comt_COM_features = [comt_BOW_tw_matrix,comt_BOW_mentionOnce_matrix,comt_BOW_mentionAllPron_matrix,comt_BOW_rmTopic_matrix,\n",
    "                comt_W2V_tw_vector, comt_W2V_topnwd_matrix, comt_W2V_topnwd_vectors, comt_W2V_topicwd_vectors]\n",
    "comt_COM_feature_dict = {}\n",
    "for i in range(len(comt_COM_feature_names)):\n",
    "    comt_COM_feature_dict[comt_COM_feature_names[i]] = comt_COM_features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine 1 features.\n",
      "Combine 2 features.\n",
      "Combine 3 features.\n",
      "Combine 4 features.\n",
      "Combine 5 features.\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_topnwd_matrix + comt_W2V_topnwd_vectors\t0.678\n",
      "comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix\t0.675\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix\t0.675\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix + comt_W2V_topnwd_vectors\t0.675\n",
      "comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix + comt_W2V_topnwd_vectors\t0.675\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_vectors\t0.675\n",
      "comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_vectors\t0.673\n",
      "comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix + comt_W2V_tw_vector + comt_W2V_topnwd_matrix + comt_W2V_topicwd_vectors\t0.671\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_W2V_tw_vector\t0.671\n",
      "comt_BOW_tw_matrix + comt_BOW_mentionOnce_matrix + comt_BOW_mentionAllPron_matrix + comt_BOW_rmTopic_matrix\t0.671\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_comb_feature(comt_COM_feature_names,comt_COM_feature_dict,comt_moclabel_list,comt_lr,mycv,score_func='f1')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.731\n",
      "recall:0.626\n",
      "f1:0.670\n"
     ]
    }
   ],
   "source": [
    "comt_com_Best_feature_names=[\"comt_BOW_tw_matrix\",\"comt_BOW_mentionAllPron_matrix\", \"comt_W2V_topnwd_vectors\"]\n",
    "comt_com_Best_feature_list = [comt_BOW_tw_matrix, comt_BOW_mentionAllPron_matrix, comt_W2V_topnwd_vectors]\n",
    "comt_com_Best_feature_dict = {}\n",
    "for i in range(len(comt_com_Best_feature_names)):\n",
    "    comt_com_Best_feature_dict[comt_com_Best_feature_names[i]] = comt_com_Best_feature_names[i]\n",
    "   \n",
    "comt_com_Best_feature = comt_com_Best_feature_list[0]\n",
    "for feature in comt_com_Best_feature_list[1:]:\n",
    "    comt_com_Best_feature = np.hstack((comt_com_Best_feature,feature))\n",
    "\n",
    "print(\"precision:%.3f\" % np.mean(cross_val_score(sup_lr, comt_com_Best_feature, comt_moclabel_list,cv=mycv,scoring='precision')))\n",
    "print(\"recall:%.3f\" % np.mean(cross_val_score(sup_lr, comt_com_Best_feature, comt_moclabel_list,cv=mycv,scoring='recall')))\n",
    "print(\"f1:%.3f\" % np.mean(cross_val_score(sup_lr, comt_com_Best_feature, comt_moclabel_list,cv=mycv,scoring='f1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[MOC-eco]** section 4.2: Evaluate different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(multi_class ='ovr',penalty='l2',class_weight=\"balanced\")\n",
    "gnb = GaussianNB()\n",
    "rf = RandomForestClassifier()\n",
    "nn = MLPClassifier(solver='lbfgs',alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908\tLogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.862\tGaussianNB(priors=None)\n",
      "0.855\tRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "0.902\tMLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(15,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "Cause.eva_classifier(sup_com_Best_feature,sup_moclabel_list,mycv,score_func='f1',classifier_list = [lr,gnb,rf,nn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MOC-eco]** section 4.3: Analyze terms that have high coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comt_BOW_lr = LogisticRegression(penalty=\"l2\",class_weight=\"balanced\")\n",
    "comt_BOW_lr.fit(comt_BOW_mentionAllPron_matrix, comt_moclabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 positive coefficient words:\n",
      "                   i\t2.490\n",
      "                  my\t1.837\n",
      "                  me\t1.453\n",
      "                must\t1.214\n",
      "             hearing\t1.124\n",
      "                 day\t1.087\n",
      "             discuss\t1.083\n",
      "                part\t1.082\n",
      "           _self_for\t1.004\n",
      "                bill\t0.980\n",
      "  _MENTION_defenders\t0.943\n",
      "               about\t0.919\n",
      "               local\t0.899\n",
      "               let's\t0.880\n",
      "             working\t0.878\n",
      "                   w\t0.851\n",
      "               areas\t0.801\n",
      "             coastal\t0.790\n",
      "         legislation\t0.742\n",
      "_HASHTAG_conservation\t0.741\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 positive coefficient words:\")\n",
    "for i in np.argsort(comt_BOW_lr.coef_[0])[::-1][:20]:\n",
    "    print('%20s\\t%.3f' % (comt_mentionAllpron_vectorizer.get_feature_names()[i], comt_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 negative coefficient words:\n",
      "                 see\t-0.952\n",
      "                  rt\t-0.942\n",
      "              global\t-0.830\n",
      "                 are\t-0.786\n",
      "           pollution\t-0.776\n",
      "            reminder\t-0.737\n",
      "                come\t-0.711\n",
      "            historic\t-0.703\n",
      "                  if\t-0.683\n",
      "               clean\t-0.649\n",
      "                  is\t-0.649\n",
      "                more\t-0.644\n",
      "       _self_species\t-0.608\n",
      "                 oil\t-0.605\n",
      "                 new\t-0.582\n",
      "                 who\t-0.576\n",
      "       _self_climate\t-0.570\n",
      "                 you\t-0.569\n",
      "             warming\t-0.567\n",
      "                time\t-0.559\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 20 negative coefficient words:\")\n",
    "for i in np.argsort(comt_BOW_lr.coef_[0])[::1][:20]:\n",
    "    print('%20s\\t%.3f' % (comt_mentionAllpron_vectorizer.get_feature_names()[i], comt_BOW_lr.coef_[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MOC-eco]** section 5: Apply pre-trained classifiers to predict for unseen tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MOC-eco]** section 5.1: Apply support classifier to classify all brands' tweets into support and non-support classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sup_lr.fit(sup_com_Best_feature,sup_moclabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cause.ecobrand_predict_label_0_1(sup_lr,ecobrand_nameid,eco_terms,GN_model,eco_keywords,sup_cont_vectorizer,\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_score_test.txt\",\n",
    "                             \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_23_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MOC-eco]** section 5.2: Apply commitment classifier to classify all brands' support tweets into high- and low- commitment classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comt_lr.fit(comt_com_Best_feature,comt_moclabel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cause.ecobrand_predict_label_2_3(comt_lr,GN_model,ecobrand_nameid,eco_terms,eco_keywords,comt_bow_vectorizer,comt_mention1_vectorizer,comt_mentionAllpron_vectorizer,comt_rmTopic_vectorizer,\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_23_test.txt\",\n",
    "                              \"/data/2/zwang/2017_S/Tweet_Eco/966brand_tweet_01_2_3_test.txt\")         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MOC-eco]** section 6: aggregate each entity's cause-commitment tweets and compare with action score to find inauthentic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get data for 510 entities\n"
     ]
    }
   ],
   "source": [
    "entity_pred_info, entity_pred2_tw, entity_pred3_tw = Cause.get_aggregate_info(\"/data/2/zwang/2017_S/Congress_Eco/514moc_tweet_predict_proba_01_2_3_re2.txt\",\n",
    "                                                         sim_limit=0.3,prob_limit=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MOC_score_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-971c12491bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mremain_entity_predicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_entity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCause\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilt_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_pred_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOC_score_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntw_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MOC_score_dict' is not defined"
     ]
    }
   ],
   "source": [
    "remain_entity_predicts, remove_entity = Cause.filt_entity(entity_pred_info, MOC_score_dict, ntw_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MOC-eco]** section 6.1: Apply different aggregation methods to select entities that have high word-ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_n3,entity_frac3,entity_prob3,words_topn_entities = Cause.aggregation(remain_entity_predicts,topn=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[MOC-eco]** section 6.2: Sort high word-rating entities by their action-rating and select top-n (high word-rating but low action-rating) as inauthentic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inauthentic_entities = Cause.inauthentic(words_topn_entities,moc_score_dict,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"entity\\taction_score\\tn_label3\\tfrac_label3\\tprob_label3\\n\")\n",
    "for entity in inauthentic_entities:\n",
    "    print(\"%s\\t%s\\t%d\\t%f\\t%f\" % (entity,ecobrand_score_dict[entity],int(entity_n3[entity]),float(entity_frac3[entity]),\n",
    "                                  float(entity_prob3[entity])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**[For all 3 datasets]** section 7. Fit linear regression model to analyze how does entities' word commitment level relate with action-ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
